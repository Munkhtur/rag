
https://huggingface.co/blog/train-sentence-transformers#training-arguments

[5e−6,1e−5,2e−5,3e−5,4e−5,5e−5,6e−5,7e−5,8e−5,9e−5,1e−4]       1e-4 < 5e-6 <  5e-5 < 3e-5 , 2e-5 

EPOCHS = 7
BATCH_SIZE = 16
LEARNING_RATE =5e-5
 [ 520/6293 05:13 < 58:18, 1.65 it/s, Epoch 0.58/7]
Here's a cleaned and tabulated view of your data for better readability:

Step	Training Loss	Validation Loss	Cosine Accuracy@1	Cosine Accuracy@3	Cosine Accuracy@5	Cosine Accuracy@10	Cosine Precision@1	Cosine Precision@3	Cosine Precision@5	Cosine Precision@10	Cosine Recall@1	Cosine Recall@3	Cosine Recall@5	Cosine Recall@10	Cosine Ndcg@10	Cosine Mrr@10	Cosine Map@100
100	No log	No log	0.324462	0.499022	0.553816	0.623092	0.324462	0.166341	0.110763	0.062309	0.324462	0.499022	0.553816	0.623092	0.474124	0.426578	0.436623
200	No log	No log	0.313894	0.495108	0.556947	0.629354	0.313894	0.165036	0.111389	0.062935	0.313894	0.495108	0.556947	0.629354	0.470899	0.420395	0.429531
300	No log	No log	0.312329	0.481018	0.538552	0.612524	0.312329	0.160339	0.107710	0.061252	0.312329	0.481018	0.538552	0.612524	0.461383	0.413349	0.423060
400	No log	No log	0.297847	0.475538	0.539335	0.625049	0.297847	0.158513	0.107867	0.062505	0.297847	0.475538	0.539335	0.625049	0.457307	0.404201	0.414087
500	2.041500	No log	0.322114	0.490802	0.551076	0.631311	0.322114	0.163601	0.110215	0.063131	0.322114	0.490802	0.551076	0.631311	0.473971	0.424138	0.432946



EPOCHS = 7
BATCH_SIZE = 16
LEARNING_RATE =1e-4

 [ 601/6293 05:59 < 56:59, 1.66 it/s, Epoch 0.67/7]
Step	Training Loss	Validation Loss	Cosine Accuracy@1	Cosine Accuracy@3	Cosine Accuracy@5	Cosine Accuracy@10	Cosine Precision@1	Cosine Precision@3	Cosine Precision@5	Cosine Precision@10	Cosine Recall@1	Cosine Recall@3	Cosine Recall@5	Cosine Recall@10	Cosine Ndcg@10	Cosine Mrr@10	Cosine Map@100
100	No log	No log	0.287280	0.443836	0.508023	0.577299	0.287280	0.147945	0.101605	0.057730	0.287280	0.443836	0.508023	0.577299	0.428486	0.381358	0.391784
200	No log	No log	0.261840	0.436791	0.502544	0.579256	0.261840	0.145597	0.100509	0.057926	0.261840	0.436791	0.502544	0.579256	0.416911	0.365380	0.375297
300	No log	No log	0.268102	0.423875	0.481409	0.566341	0.268102	0.141292	0.096282	0.056634	0.268102	0.423875	0.481409	0.566341	0.411362	0.362613	0.372895
400	No log	No log	0.230920	0.394129	0.452055	0.534247	0.230920	0.131376	0.090411	0.053425	0.230920	0.394129	0.452055	0.534247	0.377914	0.328511	0.339539
500	1.920800	No log	0.281018	0.441487	0.496282	0.571429	0.281018	0.147162	0.099256	0.057143	0.281018	0.441487	0.496282	0.571429	0.423911	0.377145	0.387735


EPOCHS = 7
BATCH_SIZE = 16
LEARNING_RATE =5e-6
 [ 501/6293 05:30 < 1:03:52, 1.51 it/s, Epoch 0.56/7]
Step	Training Loss	Validation Loss	Cosine Accuracy@1	Cosine Accuracy@3	Cosine Accuracy@5	Cosine Accuracy@10	Cosine Precision@1	Cosine Precision@3	Cosine Precision@5	Cosine Precision@10	Cosine Recall@1	Cosine Recall@3	Cosine Recall@5	Cosine Recall@10	Cosine Ndcg@10	Cosine Mrr@10	Cosine Map@100
100	No log	No log	0.270059	0.431311	0.501761	0.584736	0.270059	0.143770	0.100352	0.058474	0.270059	0.431311	0.501761	0.584736	0.422582	0.371330	0.381546
200	No log	No log	0.283366	0.444227	0.518591	0.595695	0.283366	0.148076	0.103718	0.059569	0.283366	0.444227	0.518591	0.595695	0.435147	0.384360	0.394561
300	No log	No log	0.290802	0.453620	0.522114	0.604697	0.290802	0.151207	0.104423	0.060470	0.290802	0.453620	0.522114	0.604697	0.442001	0.390718	0.400670
400	No log	No log	0.297456	0.457143	0.521722	0.602740	0.297456	0.152381	0.104344	0.060274	0.297456	0.457143	0.521722	0.602740	0.445034	0.395247	0.405679
500	0.742600	No log	0.298630	0.460665	0.524853	0.607828	0.298630	0.153555	0.104971	0.060783	0.298630	0.460665	0.524853	0.607828	0.448466	0.398129	0.408213


EPOCHS = 7
BATCH_SIZE = 16
LEARNING_RATE =3e-5
no warmup
 [ 501/6293 05:59 < 1:09:33, 1.39 it/s, Epoch 0.56/7]
Step	Training Loss	Validation Loss	Cosine Accuracy@1	Cosine Accuracy@3	Cosine Accuracy@5	Cosine Accuracy@10	Cosine Precision@1	Cosine Precision@3	Cosine Precision@5	Cosine Precision@10	Cosine Recall@1	Cosine Recall@3	Cosine Recall@5	Cosine Recall@10	Cosine Ndcg@10	Cosine Mrr@10	Cosine Map@100
100	No log	No log	0.294716	0.462231	0.524853	0.609393	0.294716	0.154077	0.104971	0.060939	0.294716	0.462231	0.524853	0.609393	0.448106	0.397108	0.407438
200	No log	No log	0.288063	0.456751	0.513894	0.594521	0.288063	0.152250	0.102779	0.059452	0.288063	0.456751	0.513894	0.594521	0.437888	0.388288	0.398473
300	No log	No log	0.293151	0.459100	0.521331	0.597260	0.293151	0.153033	0.104266	0.059726	0.293151	0.459100	0.521331	0.597260	0.440992	0.391554	0.402412
400	No log	No log	0.301370	0.472798	0.536595	0.612133	0.301370	0.157599	0.107319	0.061213	0.301370	0.472798	0.536595	0.612133	0.453989	0.403778	0.413638
500	0.242500	No log	0.313112	0.481800	0.540117	0.607045	0.313112	0.160600	0.108023	0.060705	0.313112	0.481800	0.540117	0.607045	0.459533	0.412492	0.422298


EPOCHS = 7
BATCH_SIZE = 16
LEARNING_RATE =2e-5


 [ 501/6293 05:36 < 1:05:04, 1.48 it/s, Epoch 0.56/7]
Step	Training Loss	Validation Loss	Cosine Accuracy@1	Cosine Accuracy@3	Cosine Accuracy@5	Cosine Accuracy@10	Cosine Precision@1	Cosine Precision@3	Cosine Precision@5	Cosine Precision@10	Cosine Recall@1	Cosine Recall@3	Cosine Recall@5	Cosine Recall@10	Cosine Ndcg@10	Cosine Mrr@10	Cosine Map@100
100	No log	No log	0.307241	0.482192	0.536595	0.614873	0.307241	0.160731	0.107319	0.061487	0.307241	0.482192	0.536595	0.614873	0.459349	0.410041	0.420062
200	No log	No log	0.299022	0.474364	0.539335	0.612133	0.299022	0.158121	0.107867	0.061213	0.299022	0.474364	0.539335	0.612133	0.453920	0.403595	0.414018
300	No log	No log	0.312329	0.487280	0.540117	0.612524	0.312329	0.162427	0.108023	0.061252	0.312329	0.487280	0.540117	0.612524	0.462163	0.414239	0.424631
400	No log	No log	0.309589	0.485323	0.544031	0.620352	0.309589	0.161774	0.108806	0.062035	0.309589	0.485323	0.544031	0.620352	0.463240	0.413265	0.423083
500	0.050500	No log	0.316243	0.492368	0.547162	0.616830	0.316243	0.164123	0.109432	0.061683	0.316243	0.492368	0.547162	0.616830	0.466833	0.418954	0.428720

EPOCHS = 7
BATCH_SIZE = 16
LEARNING_RATE =3e-5
WARMUP_STEPS = int(len(loader) * EPOCHS * 0.1)

 [ 628/6293 07:10 < 1:04:58, 1.45 it/s, Epoch 0.70/7]
Step	Training Loss	Validation Loss	Cosine Accuracy@1	Cosine Accuracy@3	Cosine Accuracy@5	Cosine Accuracy@10	Cosine Precision@1	Cosine Precision@3	Cosine Precision@5	Cosine Precision@10	Cosine Recall@1	Cosine Recall@3	Cosine Recall@5	Cosine Recall@10	Cosine Ndcg@10	Cosine Mrr@10	Cosine Map@100
100	No log	No log	0.313112	0.493151	0.548337	0.624658	0.313112	0.164384	0.109667	0.062466	0.313112	0.493151	0.548337	0.624658	0.468393	0.418667	0.428181
200	No log	No log	0.314286	0.496282	0.553816	0.624658	0.314286	0.165427	0.110763	0.062466	0.314286	0.496282	0.553816	0.624658	0.470093	0.420745	0.430505
300	No log	No log	0.307241	0.492759	0.547162	0.616438	0.307241	0.164253	0.109432	0.061644	0.307241	0.492759	0.547162	0.616438	0.463108	0.414000	0.424384
400	No log	No log	0.309589	0.492759	0.550685	0.621135	0.309589	0.164253	0.110137	0.062114	0.309589	0.492759	0.550685	0.621135	0.466066	0.416477	0.426465
500	0.008400	No log	0.313894	0.490020	0.537769	0.615264	0.313894	0.163340	0.107554	0.061526	0.313894	0.490020	0.537769	0.615264	0.464744	0.416840	0.426644
600	0.008400	No log	0.280235	0.452838	0.510372	0.584344	0.280235	0.150946	0.102074	0.058434	0.280235	0.452838	0.510372	0.584344	0.430012	0.380972	0.392307
---------------------------------------------------------------------------


EPOCHS = 7
BATCH_SIZE = 16
LEARNING_RATE =3e-5
WARMUP_STEPS = int(len(loader) * EPOCHS * 0.2)



 [1327/6293 15:30 < 58:09, 1.42 it/s, Epoch 1.47/7]
Step	Training Loss	Validation Loss	Cosine Accuracy@1	Cosine Accuracy@3	Cosine Accuracy@5	Cosine Accuracy@10	Cosine Precision@1	Cosine Precision@3	Cosine Precision@5	Cosine Precision@10	Cosine Recall@1	Cosine Recall@3	Cosine Recall@5	Cosine Recall@10	Cosine Ndcg@10	Cosine Mrr@10	Cosine Map@100
100	No log	No log	0.289237	0.446967	0.514677	0.594521	0.289237	0.148989	0.102935	0.059452	0.289237	0.446967	0.514677	0.594521	0.436434	0.386600	0.397366
200	No log	No log	0.302544	0.464579	0.523679	0.607436	0.302544	0.154860	0.104736	0.060744	0.302544	0.464579	0.523679	0.607436	0.450564	0.401103	0.411622
300	No log	No log	0.309980	0.466536	0.532290	0.610176	0.309980	0.155512	0.106458	0.061018	0.309980	0.466536	0.532290	0.610176	0.456265	0.407612	0.418259
400	No log	No log	0.310763	0.468102	0.538943	0.618004	0.310763	0.156034	0.107789	0.061800	0.310763	0.468102	0.538943	0.618004	0.459873	0.409920	0.420074
500	0.030800	No log	0.316634	0.481800	0.540900	0.616830	0.316634	0.160600	0.108180	0.061683	0.316634	0.481800	0.540900	0.616830	0.464017	0.415567	0.425914
600	0.030800	No log	0.311155	0.473973	0.537378	0.609785	0.311155	0.157991	0.107476	0.060978	0.311155	0.473973	0.537378	0.609785	0.457766	0.409513	0.420515
700	0.030800	No log	0.291977	0.466928	0.520157	0.599217	0.291977	0.155643	0.104031	0.059922	0.291977	0.466928	0.520157	0.599217	0.443186	0.393631	0.404799
800	0.030800	No log	0.309980	0.475538	0.536595	0.613699	0.309980	0.158513	0.107319	0.061370	0.309980	0.475538	0.536595	0.613699	0.458738	0.409647	0.419929
899	0.030800	No log	0.316634	0.488845	0.543640	0.620352	0.316634	0.162948	0.108728	0.062035	0.316634	0.488845	0.543640	0.620352	0.467048	0.418381	0.428040
900	0.030800	No log	0.318982	0.488454	0.542074	0.620744	0.318982	0.162818	0.108415	0.062074	0.318982	0.488454	0.542074	0.620744	0.467892	0.419432	0.429072
1000	1.265000	No log	0.320548	0.490411	0.544814	0.620352	0.320548	0.163470	0.108963	0.062035	0.320548	0.490411	0.544814	0.620352	0.468910	0.420806	0.431152
1100	1.265000	No log	0.319374	0.481409	0.533855	0.612524	0.319374	0.160470	0.106771	0.061252	0.319374	0.481409	0.533855	0.612524	0.463810	0.416659	0.426377
1200	1.265000	No log	0.293151	0.468493	0.533464	0.610959	0.293151	0.156164	0.106693	0.061096	0.293151	0.468493	0.533464	0.610959	0.449827	0.398696	0.409133
1300	1.265000	No log	0.306458	0.479061	0.540117	0.612133	0.306458	0.159687	0.108023	0.061213	0.306458	0.479061	0.540117	0.612133	0.458070	0.409056	0.419471
---------------------------------------------------------------------------

EPOCHS = 7
BATCH_SIZE = 16
LEARNING_RATE =3e-5
outpath = 'finetuned_paraphrase-multilingual_mpnet_try5'

loader = DataLoader(
    examples, batch_size=BATCH_SIZE
)
WARMUP_STEPS = int(len(loader) * EPOCHS * 0.1)



[2040/6293 24:23 < 50:54, 1.39 it/s, Epoch 2.27/7]
Step 	Training Loss 	Validation Loss 	Cosine Accuracy@1 	Cosine Accuracy@3 	Cosine Accuracy@5 	Cosine Accuracy@10 	Cosine Precision@1 	Cosine Precision@3 	Cosine Precision@5 	Cosine Precision@10 	Cosine Recall@1 	Cosine Recall@3 	Cosine Recall@5 	Cosine Recall@10 	Cosine Ndcg@10 	Cosine Mrr@10 	Cosine Map@100
100 	No log 	No log 	0.268102 	0.416438 	0.486106 	0.575342 	0.268102 	0.138813 	0.097221 	0.057534 	0.268102 	0.416438 	0.486106 	0.575342 	0.413555 	0.362811 	0.373267
200 	No log 	No log 	0.300978 	0.467710 	0.527593 	0.618395 	0.300978 	0.155903 	0.105519 	0.061840 	0.300978 	0.467710 	0.527593 	0.618395 	0.454652 	0.403074 	0.412194
300 	No log 	No log 	0.306849 	0.476321 	0.531507 	0.600391 	0.306849 	0.158774 	0.106301 	0.060039 	0.306849 	0.476321 	0.531507 	0.600391 	0.453652 	0.406849 	0.417018
400 	No log 	No log 	0.326027 	0.495499 	0.560861 	0.633268 	0.326027 	0.165166 	0.112172 	0.063327 	0.326027 	0.495499 	0.560861 	0.633268 	0.477535 	0.428067 	0.437342
500 	4.771700 	No log 	0.328376 	0.498630 	0.550685 	0.620744 	0.328376 	0.166210 	0.110137 	0.062074 	0.328376 	0.498630 	0.550685 	0.620744 	0.475210 	0.428770 	0.438202
600 	4.771700 	No log 	0.295499 	0.472407 	0.533072 	0.609393 	0.295499 	0.157469 	0.106614 	0.060939 	0.295499 	0.472407 	0.533072 	0.609393 	0.450626 	0.400157 	0.410791
700 	4.771700 	No log 	0.322505 	0.500587 	0.557730 	0.628963 	0.322505 	0.166862 	0.111546 	0.062896 	0.322505 	0.500587 	0.557730 	0.628963 	0.475962 	0.427143 	0.436567
800 	4.771700 	No log 	0.324070 	0.500587 	0.564384 	0.636399 	0.324070 	0.166862 	0.112877 	0.063640 	0.324070 	0.500587 	0.564384 	0.636399 	0.480067 	0.430166 	0.440096
899 	4.771700 	No log 	0.336986 	0.513503 	0.572211 	0.645010 	0.336986 	0.171168 	0.114442 	0.064501 	0.336986 	0.513503 	0.572211 	0.645010 	0.490713 	0.441523 	0.450304
900 	4.771700 	No log 	0.338552 	0.515068 	0.571429 	0.645793 	0.338552 	0.171689 	0.114286 	0.064579 	0.338552 	0.515068 	0.571429 	0.645793 	0.491882 	0.442817 	0.451488
1000 	3.437400 	No log 	0.326027 	0.508023 	0.567906 	0.637965 	0.326027 	0.169341 	0.113581 	0.063796 	0.326027 	0.508023 	0.567906 	0.637965 	0.481655 	0.431727 	0.440522
1100 	3.437400 	No log 	0.332290 	0.508806 	0.562818 	0.632877 	0.332290 	0.169602 	0.112564 	0.063288 	0.332290 	0.508806 	0.562818 	0.632877 	0.482666 	0.434692 	0.443146
1200 	3.437400 	No log 	0.338552 	0.514286 	0.576517 	0.647750 	0.338552 	0.171429 	0.115303 	0.064775 	0.338552 	0.514286 	0.576517 	0.647750 	0.492943 	0.443517 	0.452161
1300 	3.437400 	No log 	0.338943 	0.515460 	0.574560 	0.643053 	0.338943 	0.171820 	0.114912 	0.064305 	0.338943 	0.515460 	0.574560 	0.643053 	0.490749 	0.442104 	0.450913
1400 	3.437400 	No log 	0.334247 	0.513894 	0.573386 	0.646575 	0.334247 	0.171298 	0.114677 	0.064658 	0.334247 	0.513894 	0.573386 	0.646575 	0.489520 	0.439513 	0.448500
1500 	2.448600 	No log 	0.338943 	0.520939 	0.576125 	0.647750 	0.338943 	0.173646 	0.115225 	0.064775 	0.338943 	0.520939 	0.576125 	0.647750 	0.493559 	0.444333 	0.453687
1600 	2.448600 	No log 	0.326810 	0.509589 	0.569080 	0.650489 	0.326810 	0.169863 	0.113816 	0.065049 	0.326810 	0.509589 	0.569080 	0.650489 	0.485945 	0.433708 	0.443103
1700 	2.448600 	No log 	0.348337 	0.527202 	0.577691 	0.652446 	0.348337 	0.175734 	0.115538 	0.065245 	0.348337 	0.527202 	0.577691 	0.652446 	0.499888 	0.451312 	0.460660
1798 	2.448600 	No log 	0.332290 	0.507632 	0.574560 	0.647358 	0.332290 	0.169211 	0.114912 	0.064736 	0.332290 	0.507632 	0.574560 	0.647358 	0.487733 	0.437001 	0.446338
1800 	2.448600 	No log 	0.335029 	0.508806 	0.578474 	0.647358 	0.335029 	0.169602 	0.115695 	0.064736 	0.335029 	0.508806 	0.578474 	0.647358 	0.489856 	0.439700 	0.449069
1900 	2.448600 	No log 	0.349511 	0.526810 	0.579256 	0.653229 	0.349511 	0.175603 	0.115851 	0.065323 	0.349511 	0.526810 	0.579256 	0.653229 	0.500666 	0.452025 	0.461143
2000 	1.861000 	No log 	0.342857 	0.515460 	0.565558 	0.639922 	0.342857 	0.171820 	0.113112 	0.063992 	0.342857 	0.515460 	0.565558 	0.639922 	0.491288 	0.443932 	0.452650

EPOCHS = 10
BATCH_SIZE = 16
LEARNING_RATE =3e-5
outpath = 'finetuned_paraphrase-multilingual_mpnet_try5'

loader = DataLoader(
    examples, batch_size=BATCH_SIZE
)
WARMUP_STEPS = int(len(loader) * EPOCHS * 0.1)




[8990/8990 1:40:07, Epoch 10/10]
Step 	Training Loss 	Validation Loss 	Cosine Accuracy@1 	Cosine Accuracy@3 	Cosine Accuracy@5 	Cosine Accuracy@10 	Cosine Precision@1 	Cosine Precision@3 	Cosine Precision@5 	Cosine Precision@10 	Cosine Recall@1 	Cosine Recall@3 	Cosine Recall@5 	Cosine Recall@10 	Cosine Ndcg@10 	Cosine Mrr@10 	Cosine Map@100
100 	No log 	No log 	0.344031 	0.524462 	0.581605 	0.654795 	0.344031 	0.174821 	0.116321 	0.065479 	0.344031 	0.524462 	0.581605 	0.654795 	0.498656 	0.448861 	0.457648
200 	No log 	No log 	0.351468 	0.521722 	0.589824 	0.659100 	0.351468 	0.173907 	0.117965 	0.065910 	0.351468 	0.521722 	0.589824 	0.659100 	0.504189 	0.454817 	0.463500
300 	No log 	No log 	0.344031 	0.521722 	0.580039 	0.650098 	0.344031 	0.173907 	0.116008 	0.065010 	0.344031 	0.521722 	0.580039 	0.650098 	0.496581 	0.447583 	0.456593
400 	No log 	No log 	0.340509 	0.520157 	0.581213 	0.650881 	0.340509 	0.173386 	0.116243 	0.065088 	0.340509 	0.520157 	0.581213 	0.650881 	0.495702 	0.446134 	0.455424
500 	1.768800 	No log 	0.336595 	0.505675 	0.569472 	0.639530 	0.336595 	0.168558 	0.113894 	0.063953 	0.336595 	0.505675 	0.569472 	0.639530 	0.486856 	0.438200 	0.447062
600 	1.768800 	No log 	0.333072 	0.504110 	0.556164 	0.630528 	0.333072 	0.168037 	0.111233 	0.063053 	0.333072 	0.504110 	0.556164 	0.630528 	0.481319 	0.433783 	0.443733
700 	1.768800 	No log 	0.338552 	0.506067 	0.561252 	0.640705 	0.338552 	0.168689 	0.112250 	0.064070 	0.338552 	0.506067 	0.561252 	0.640705 	0.487962 	0.439499 	0.448819
800 	1.768800 	No log 	0.326027 	0.497847 	0.549902 	0.617221 	0.326027 	0.165949 	0.109980 	0.061722 	0.326027 	0.497847 	0.549902 	0.617221 	0.472221 	0.425906 	0.434994
899 	1.768800 	No log 	0.328376 	0.495108 	0.560470 	0.637573 	0.328376 	0.165036 	0.112094 	0.063757 	0.328376 	0.495108 	0.560470 	0.637573 	0.479857 	0.429906 	0.439575
900 	1.768800 	No log 	0.328376 	0.495890 	0.559687 	0.636791 	0.328376 	0.165297 	0.111937 	0.063679 	0.328376 	0.495890 	0.559687 	0.636791 	0.479766 	0.430000 	0.439840
1000 	0.762400 	No log 	0.331115 	0.510372 	0.573777 	0.636008 	0.331115 	0.170124 	0.114755 	0.063601 	0.331115 	0.510372 	0.573777 	0.636008 	0.484766 	0.436230 	0.444986
1100 	0.762400 	No log 	0.320157 	0.505675 	0.566341 	0.641487 	0.320157 	0.168558 	0.113268 	0.064149 	0.320157 	0.505675 	0.566341 	0.641487 	0.479611 	0.428109 	0.437394
1200 	0.762400 	No log 	0.307241 	0.486497 	0.544814 	0.628571 	0.307241 	0.162166 	0.108963 	0.062857 	0.307241 	0.486497 	0.544814 	0.628571 	0.465251 	0.413496 	0.423328
1300 	0.762400 	No log 	0.322896 	0.497065 	0.556947 	0.632877 	0.322896 	0.165688 	0.111389 	0.063288 	0.322896 	0.497065 	0.556947 	0.632877 	0.476290 	0.426468 	0.435912
1400 	0.762400 	No log 	0.324070 	0.500196 	0.556556 	0.637182 	0.324070 	0.166732 	0.111311 	0.063718 	0.324070 	0.500196 	0.556556 	0.637182 	0.478753 	0.428470 	0.437826
1500 	1.560500 	No log 	0.324853 	0.496673 	0.559295 	0.636791 	0.324853 	0.165558 	0.111859 	0.063679 	0.324853 	0.496673 	0.559295 	0.636791 	0.478118 	0.427810 	0.437596
1600 	1.560500 	No log 	0.324070 	0.509589 	0.568297 	0.641879 	0.324070 	0.169863 	0.113659 	0.064188 	0.324070 	0.509589 	0.568297 	0.641879 	0.483123 	0.432438 	0.442206
1700 	1.560500 	No log 	0.329550 	0.522896 	0.578865 	0.649315 	0.329550 	0.174299 	0.115773 	0.064932 	0.329550 	0.522896 	0.578865 	0.649315 	0.490501 	0.439726 	0.448704
1798 	1.560500 	No log 	0.318591 	0.506458 	0.567906 	0.638748 	0.318591 	0.168819 	0.113581 	0.063875 	0.318591 	0.506458 	0.567906 	0.638748 	0.479018 	0.427987 	0.437779
1800 	1.560500 	No log 	0.320939 	0.503718 	0.566341 	0.636399 	0.320939 	0.167906 	0.113268 	0.063640 	0.320939 	0.503718 	0.566341 	0.636399 	0.478734 	0.428405 	0.438346
1900 	1.560500 	No log 	0.337769 	0.515851 	0.574951 	0.646184 	0.337769 	0.171950 	0.114990 	0.064618 	0.337769 	0.515851 	0.574951 	0.646184 	0.492413 	0.443281 	0.452339
2000 	1.683300 	No log 	0.337769 	0.506067 	0.567906 	0.637965 	0.337769 	0.168689 	0.113581 	0.063796 	0.337769 	0.506067 	0.567906 	0.637965 	0.486631 	0.438425 	0.448219
2100 	1.683300 	No log 	0.337378 	0.508806 	0.569472 	0.647358 	0.337378 	0.169602 	0.113894 	0.064736 	0.337378 	0.508806 	0.569472 	0.647358 	0.489884 	0.439937 	0.448454
2200 	1.683300 	No log 	0.329550 	0.505675 	0.560861 	0.640705 	0.329550 	0.168558 	0.112172 	0.064070 	0.329550 	0.505675 	0.560861 	0.640705 	0.483277 	0.433361 	0.442717
2300 	1.683300 	No log 	0.320548 	0.498239 	0.557730 	0.634834 	0.320548 	0.166080 	0.111546 	0.063483 	0.320548 	0.498239 	0.557730 	0.634834 	0.476208 	0.425772 	0.435125
2400 	1.683300 	No log 	0.328767 	0.498239 	0.560470 	0.640705 	0.328767 	0.166080 	0.112094 	0.064070 	0.328767 	0.498239 	0.560470 	0.640705 	0.481925 	0.431651 	0.441153
2500 	1.219600 	No log 	0.336986 	0.509980 	0.565949 	0.636399 	0.336986 	0.169993 	0.113190 	0.063640 	0.336986 	0.509980 	0.565949 	0.636399 	0.486150 	0.438289 	0.448191
2600 	1.219600 	No log 	0.333072 	0.506849 	0.567906 	0.645793 	0.333072 	0.168950 	0.113581 	0.064579 	0.333072 	0.506849 	0.567906 	0.645793 	0.487523 	0.437231 	0.446206
2697 	1.219600 	No log 	0.340509 	0.512720 	0.572603 	0.646184 	0.340509 	0.170907 	0.114521 	0.064618 	0.340509 	0.512720 	0.572603 	0.646184 	0.491220 	0.441929 	0.451246
2700 	1.219600 	No log 	0.338552 	0.514677 	0.569863 	0.648924 	0.338552 	0.171559 	0.113973 	0.064892 	0.338552 	0.514677 	0.569863 	0.648924 	0.491647 	0.441684 	0.450782
2800 	1.219600 	No log 	0.336986 	0.513894 	0.560861 	0.632877 	0.336986 	0.171298 	0.112172 	0.063288 	0.336986 	0.513894 	0.560861 	0.632877 	0.485013 	0.437835 	0.447475
2900 	1.219600 	No log 	0.334247 	0.515460 	0.574560 	0.644227 	0.334247 	0.171820 	0.114912 	0.064423 	0.334247 	0.515460 	0.574560 	0.644227 	0.488885 	0.439324 	0.448663
3000 	0.996400 	No log 	0.333855 	0.513894 	0.572211 	0.641879 	0.333855 	0.171298 	0.114442 	0.064188 	0.333855 	0.513894 	0.572211 	0.641879 	0.488698 	0.439687 	0.449272
3100 	0.996400 	No log 	0.336595 	0.513503 	0.572211 	0.643836 	0.336595 	0.171168 	0.114442 	0.064384 	0.336595 	0.513503 	0.572211 	0.643836 	0.489234 	0.439968 	0.449315
3200 	0.996400 	No log 	0.336595 	0.512720 	0.582779 	0.652446 	0.336595 	0.170907 	0.116556 	0.065245 	0.336595 	0.512720 	0.582779 	0.652446 	0.493058 	0.442250 	0.451068
3300 	0.996400 	No log 	0.345205 	0.526027 	0.588258 	0.656360 	0.345205 	0.175342 	0.117652 	0.065636 	0.345205 	0.526027 	0.588258 	0.656360 	0.501020 	0.451392 	0.460330
3400 	0.996400 	No log 	0.340117 	0.511937 	0.567515 	0.645401 	0.340117 	0.170646 	0.113503 	0.064540 	0.340117 	0.511937 	0.567515 	0.645401 	0.490463 	0.441252 	0.451277
3500 	0.802300 	No log 	0.345597 	0.519765 	0.575342 	0.645793 	0.345597 	0.173255 	0.115068 	0.064579 	0.345597 	0.519765 	0.575342 	0.645793 	0.495244 	0.447204 	0.456547
3596 	0.802300 	No log 	0.327984 	0.517417 	0.578082 	0.654012 	0.327984 	0.172472 	0.115616 	0.065401 	0.327984 	0.517417 	0.578082 	0.654012 	0.490114 	0.437861 	0.446561
3600 	0.802300 	No log 	0.331898 	0.520939 	0.579256 	0.653229 	0.331898 	0.173646 	0.115851 	0.065323 	0.331898 	0.520939 	0.579256 	0.653229 	0.492057 	0.440613 	0.449386
3700 	0.802300 	No log 	0.334638 	0.517025 	0.580822 	0.653620 	0.334638 	0.172342 	0.116164 	0.065362 	0.334638 	0.517025 	0.580822 	0.653620 	0.492644 	0.441347 	0.450348
3800 	0.802300 	No log 	0.334638 	0.518982 	0.583953 	0.653620 	0.334638 	0.172994 	0.116791 	0.065362 	0.334638 	0.518982 	0.583953 	0.653620 	0.493869 	0.442824 	0.451592
3900 	0.802300 	No log 	0.333855 	0.511937 	0.581996 	0.648141 	0.333855 	0.170646 	0.116399 	0.064814 	0.333855 	0.511937 	0.581996 	0.648141 	0.490785 	0.440425 	0.449989
4000 	0.641200 	No log 	0.331115 	0.513503 	0.580822 	0.650881 	0.331115 	0.171168 	0.116164 	0.065088 	0.331115 	0.513503 	0.580822 	0.650881 	0.490724 	0.439576 	0.448275
4100 	0.641200 	No log 	0.336204 	0.520548 	0.576125 	0.647750 	0.336204 	0.173516 	0.115225 	0.064775 	0.336204 	0.520548 	0.576125 	0.647750 	0.492315 	0.442629 	0.451618
4200 	0.641200 	No log 	0.333464 	0.519374 	0.579648 	0.643053 	0.333464 	0.173125 	0.115930 	0.064305 	0.333464 	0.519374 	0.579648 	0.643053 	0.489717 	0.440568 	0.449839
4300 	0.641200 	No log 	0.337378 	0.523679 	0.579648 	0.654403 	0.337378 	0.174560 	0.115930 	0.065440 	0.337378 	0.523679 	0.579648 	0.654403 	0.496317 	0.445849 	0.454365
4400 	0.641200 	No log 	0.346771 	0.515460 	0.576125 	0.647358 	0.346771 	0.171820 	0.115225 	0.064736 	0.346771 	0.515460 	0.576125 	0.647358 	0.496438 	0.448379 	0.457501
4495 	0.641200 	No log 	0.336986 	0.514286 	0.569080 	0.647750 	0.336986 	0.171429 	0.113816 	0.064775 	0.336986 	0.514286 	0.569080 	0.647750 	0.490312 	0.440294 	0.449521
4500 	0.574900 	No log 	0.337378 	0.512329 	0.571037 	0.645401 	0.337378 	0.170776 	0.114207 	0.064540 	0.337378 	0.512329 	0.571037 	0.645401 	0.489700 	0.440144 	0.449594
4600 	0.574900 	No log 	0.322896 	0.507241 	0.565949 	0.638748 	0.322896 	0.169080 	0.113190 	0.063875 	0.322896 	0.507241 	0.565949 	0.638748 	0.480648 	0.430210 	0.439389
4700 	0.574900 	No log 	0.329550 	0.509198 	0.568297 	0.641487 	0.329550 	0.169733 	0.113659 	0.064149 	0.329550 	0.509198 	0.568297 	0.641487 	0.484176 	0.434018 	0.443043
4800 	0.574900 	No log 	0.330333 	0.514677 	0.574951 	0.651663 	0.330333 	0.171559 	0.114990 	0.065166 	0.330333 	0.514677 	0.574951 	0.651663 	0.490178 	0.438730 	0.447789
4900 	0.574900 	No log 	0.336204 	0.506067 	0.566732 	0.650881 	0.336204 	0.168689 	0.113346 	0.065088 	0.336204 	0.506067 	0.566732 	0.650881 	0.489908 	0.439031 	0.448398
5000 	0.363000 	No log 	0.333072 	0.509198 	0.561644 	0.639530 	0.333072 	0.169733 	0.112329 	0.063953 	0.333072 	0.509198 	0.561644 	0.639530 	0.485810 	0.436842 	0.446123
5100 	0.363000 	No log 	0.330333 	0.518591 	0.572994 	0.642270 	0.330333 	0.172864 	0.114599 	0.064227 	0.330333 	0.518591 	0.572994 	0.642270 	0.486939 	0.437217 	0.446834
5200 	0.363000 	No log 	0.331115 	0.520548 	0.582387 	0.656360 	0.331115 	0.173516 	0.116477 	0.065636 	0.331115 	0.520548 	0.582387 	0.656360 	0.492827 	0.440741 	0.449856
5300 	0.363000 	No log 	0.340509 	0.518200 	0.574168 	0.652055 	0.340509 	0.172733 	0.114834 	0.065205 	0.340509 	0.518200 	0.574168 	0.652055 	0.495611 	0.445828 	0.455461
5394 	0.363000 	No log 	0.341683 	0.529941 	0.582387 	0.657143 	0.341683 	0.176647 	0.116477 	0.065714 	0.341683 	0.529941 	0.582387 	0.657143 	0.499743 	0.449485 	0.458781
5400 	0.363000 	No log 	0.345205 	0.529159 	0.586301 	0.657534 	0.345205 	0.176386 	0.117260 	0.065753 	0.345205 	0.529159 	0.586301 	0.657534 	0.501478 	0.451664 	0.460899
5500 	0.406300 	No log 	0.345988 	0.524853 	0.590607 	0.666928 	0.345988 	0.174951 	0.118121 	0.066693 	0.345988 	0.524853 	0.590607 	0.666928 	0.504245 	0.452533 	0.461169
5600 	0.406300 	No log 	0.344031 	0.527593 	0.581213 	0.657534 	0.344031 	0.175864 	0.116243 	0.065753 	0.344031 	0.527593 	0.581213 	0.657534 	0.499802 	0.449554 	0.459127
5700 	0.406300 	No log 	0.346771 	0.525245 	0.586301 	0.660274 	0.346771 	0.175082 	0.117260 	0.066027 	0.346771 	0.525245 	0.586301 	0.660274 	0.502471 	0.452215 	0.461573
5800 	0.406300 	No log 	0.343249 	0.527984 	0.586693 	0.660665 	0.343249 	0.175995 	0.117339 	0.066067 	0.343249 	0.527984 	0.586693 	0.660665 	0.501312 	0.450505 	0.459635
5900 	0.406300 	No log 	0.349511 	0.528767 	0.590215 	0.669276 	0.349511 	0.176256 	0.118043 	0.066928 	0.349511 	0.528767 	0.590215 	0.669276 	0.506507 	0.454830 	0.463505
6000 	0.322300 	No log 	0.352250 	0.523679 	0.582387 	0.661448 	0.352250 	0.174560 	0.116477 	0.066145 	0.352250 	0.523679 	0.582387 	0.661448 	0.504133 	0.454232 	0.463109
6100 	0.322300 	No log 	0.345205 	0.523679 	0.592172 	0.664971 	0.345205 	0.174560 	0.118434 	0.066497 	0.345205 	0.523679 	0.592172 	0.664971 	0.502261 	0.450535 	0.459256
6200 	0.322300 	No log 	0.342857 	0.520548 	0.586301 	0.659491 	0.342857 	0.173516 	0.117260 	0.065949 	0.342857 	0.520548 	0.586301 	0.659491 	0.498928 	0.447783 	0.457053
6293 	0.322300 	No log 	0.345597 	0.526027 	0.586301 	0.657926 	0.345597 	0.175342 	0.117260 	0.065793 	0.345597 	0.526027 	0.586301 	0.657926 	0.500600 	0.450416 	0.459856
6300 	0.322300 	No log 	0.344031 	0.527593 	0.590215 	0.660665 	0.344031 	0.175864 	0.118043 	0.066067 	0.344031 	0.527593 	0.590215 	0.660665 	0.501118 	0.450241 	0.459550
6400 	0.322300 	No log 	0.345988 	0.531115 	0.590607 	0.667710 	0.345988 	0.177038 	0.118121 	0.066771 	0.345988 	0.531115 	0.590607 	0.667710 	0.505318 	0.453670 	0.462437
6500 	0.272100 	No log 	0.349511 	0.527202 	0.584736 	0.661840 	0.349511 	0.175734 	0.116947 	0.066184 	0.349511 	0.527202 	0.584736 	0.661840 	0.504191 	0.454108 	0.463129
6600 	0.272100 	No log 	0.345988 	0.527593 	0.584344 	0.663405 	0.345988 	0.175864 	0.116869 	0.066341 	0.345988 	0.527593 	0.584344 	0.663405 	0.503627 	0.452804 	0.461638
6700 	0.272100 	No log 	0.345205 	0.527202 	0.585519 	0.660665 	0.345205 	0.175734 	0.117104 	0.066067 	0.345205 	0.527202 	0.585519 	0.660665 	0.502357 	0.451900 	0.461028
6800 	0.272100 	No log 	0.341292 	0.526027 	0.586693 	0.659883 	0.341292 	0.175342 	0.117339 	0.065988 	0.341292 	0.526027 	0.586693 	0.659883 	0.499973 	0.448969 	0.458480
6900 	0.272100 	No log 	0.344814 	0.526419 	0.587084 	0.659491 	0.344814 	0.175473 	0.117417 	0.065949 	0.344814 	0.526419 	0.587084 	0.659491 	0.501947 	0.451683 	0.461087
7000 	0.224900 	No log 	0.354599 	0.530333 	0.590998 	0.664579 	0.354599 	0.176778 	0.118200 	0.066458 	0.354599 	0.530333 	0.590998 	0.664579 	0.509038 	0.459494 	0.468562
7100 	0.224900 	No log 	0.354599 	0.531898 	0.587476 	0.667319 	0.354599 	0.177299 	0.117495 	0.066732 	0.354599 	0.531898 	0.587476 	0.667319 	0.509478 	0.459350 	0.468523
7192 	0.224900 	No log 	0.352250 	0.530333 	0.594129 	0.663405 	0.352250 	0.176778 	0.118826 	0.066341 	0.352250 	0.530333 	0.594129 	0.663405 	0.506935 	0.456996 	0.466303
7200 	0.224900 	No log 	0.349902 	0.529941 	0.594912 	0.664971 	0.349902 	0.176647 	0.118982 	0.066497 	0.349902 	0.529941 	0.594912 	0.664971 	0.506399 	0.455826 	0.464972
7300 	0.224900 	No log 	0.352250 	0.527202 	0.589041 	0.660665 	0.352250 	0.175734 	0.117808 	0.066067 	0.352250 	0.527202 	0.589041 	0.660665 	0.505101 	0.455561 	0.464874
7400 	0.224900 	No log 	0.351859 	0.526027 	0.590998 	0.663014 	0.351859 	0.175342 	0.118200 	0.066301 	0.351859 	0.526027 	0.590998 	0.663014 	0.505799 	0.455781 	0.465015
7500 	0.198200 	No log 	0.352642 	0.524853 	0.585519 	0.663014 	0.352642 	0.174951 	0.117104 	0.066301 	0.352642 	0.524853 	0.585519 	0.663014 	0.505915 	0.455985 	0.465188
7600 	0.198200 	No log 	0.351859 	0.524070 	0.586301 	0.662622 	0.351859 	0.174690 	0.117260 	0.066262 	0.351859 	0.524070 	0.586301 	0.662622 	0.505169 	0.455134 	0.464561
7700 	0.198200 	No log 	0.356164 	0.526027 	0.590607 	0.666145 	0.356164 	0.175342 	0.118121 	0.066614 	0.356164 	0.526027 	0.590607 	0.666145 	0.508529 	0.458505 	0.467617
7800 	0.198200 	No log 	0.354207 	0.525245 	0.586301 	0.673190 	0.354207 	0.175082 	0.117260 	0.067319 	0.354207 	0.525245 	0.586301 	0.673190 	0.509918 	0.458318 	0.466974
7900 	0.198200 	No log 	0.351076 	0.524853 	0.593346 	0.669276 	0.351076 	0.174951 	0.118669 	0.066928 	0.351076 	0.524853 	0.593346 	0.669276 	0.507652 	0.456375 	0.465340
8000 	0.193200 	No log 	0.351859 	0.527593 	0.588258 	0.664579 	0.351859 	0.175864 	0.117652 	0.066458 	0.351859 	0.527593 	0.588258 	0.664579 	0.506631 	0.456419 	0.465664
8091 	0.193200 	No log 	0.354990 	0.531507 	0.589824 	0.669276 	0.354990 	0.177169 	0.117965 	0.066928 	0.354990 	0.531507 	0.589824 	0.669276 	0.510236 	0.459754 	0.468424
8100 	0.193200 	No log 	0.356556 	0.530333 	0.590998 	0.668493 	0.356556 	0.176778 	0.118200 	0.066849 	0.356556 	0.530333 	0.590998 	0.668493 	0.510504 	0.460320 	0.469104
8200 	0.193200 	No log 	0.354990 	0.530333 	0.590215 	0.667319 	0.354990 	0.176778 	0.118043 	0.066732 	0.354990 	0.530333 	0.590215 	0.667319 	0.509465 	0.459282 	0.468187
8300 	0.193200 	No log 	0.354990 	0.531115 	0.589824 	0.668102 	0.354990 	0.177038 	0.117965 	0.066810 	0.354990 	0.531115 	0.589824 	0.668102 	0.509801 	0.459523 	0.468256
8400 	0.193200 	No log 	0.354599 	0.531115 	0.594129 	0.666145 	0.354599 	0.177038 	0.118826 	0.066614 	0.354599 	0.531115 	0.594129 	0.666145 	0.509106 	0.459103 	0.468003
8500 	0.160700 	No log 	0.356947 	0.532681 	0.592172 	0.663796 	0.356947 	0.177560 	0.118434 	0.066380 	0.356947 	0.532681 	0.592172 	0.663796 	0.509738 	0.460625 	0.469682
8600 	0.160700 	No log 	0.354207 	0.532681 	0.589824 	0.664188 	0.354207 	0.177560 	0.117965 	0.066419 	0.354207 	0.532681 	0.589824 	0.664188 	0.508306 	0.458656 	0.467655
8700 	0.160700 	No log 	0.351859 	0.533072 	0.591781 	0.667710 	0.351859 	0.177691 	0.118356 	0.066771 	0.351859 	0.533072 	0.591781 	0.667710 	0.508669 	0.458057 	0.466790
8800 	0.160700 	No log 	0.353816 	0.531507 	0.589824 	0.666536 	0.353816 	0.177169 	0.117965 	0.066654 	0.353816 	0.531507 	0.589824 	0.666536 	0.508894 	0.458753 	0.467637
8900 	0.160700 	No log 	0.353033 	0.530333 	0.589824 	0.667319 	0.353033 	0.176778 	0.117965 	0.066732 	0.353033 	0.530333 	0.589824 	0.667319 	0.508765 	0.458312 	0.467132
8990 	0.160700 	No log 	0.353425 	0.529941 	0.590215 	0.667319 	0.353425 	0.176647 	0.118043 	0.066732 	0.353425 	0.529941 	0.590215 	0.667319 	0.508859 	0.458451 	0.467260

—----------------------------------------------------------------------------------------------

EPOCHS = 10
BATCH_SIZE = 16
LEARNING_RATE =2e-5
outpath = 'finetuned_paraphrase-multilingual_mpnet_try6'

loader = DataLoader(
    examples, batch_size=BATCH_SIZE
)
WARMUP_STEPS = int(len(loader) * EPOCHS * 0.1)


 [8990/8990 1:36:14, Epoch 10/10]
Step	Training Loss	Validation Loss	Cosine Accuracy@1	Cosine Accuracy@3	Cosine Accuracy@5	Cosine Accuracy@10	Cosine Precision@1	Cosine Precision@3	Cosine Precision@5	Cosine Precision@10	Cosine Recall@1	Cosine Recall@3	Cosine Recall@5	Cosine Recall@10	Cosine Ndcg@10	Cosine Mrr@10	Cosine Map@100
100	No log	No log	0.216047	0.373777	0.441096	0.522896	0.216047	0.124592	0.088219	0.052290	0.216047	0.373777	0.441096	0.522896	0.362783	0.312348	0.324276
200	No log	No log	0.293151	0.452838	0.511155	0.598826	0.293151	0.150946	0.102231	0.059883	0.293151	0.452838	0.511155	0.598826	0.441170	0.391441	0.401280
300	No log	No log	0.299804	0.463405	0.519374	0.598043	0.299804	0.154468	0.103875	0.059804	0.299804	0.463405	0.519374	0.598043	0.445999	0.397888	0.408042
400	No log	No log	0.326810	0.495890	0.548728	0.623875	0.326810	0.165297	0.109746	0.062387	0.326810	0.495890	0.548728	0.623875	0.474785	0.427438	0.436930
500	5.154800	No log	0.334247	0.499804	0.547554	0.627006	0.334247	0.166601	0.109511	0.062701	0.334247	0.499804	0.547554	0.627006	0.479520	0.432725	0.441800
600	5.154800	No log	0.324070	0.500978	0.556947	0.636791	0.324070	0.166993	0.111389	0.063679	0.324070	0.500978	0.556947	0.636791	0.478633	0.428458	0.437846
700	5.154800	No log	0.336986	0.510763	0.568689	0.642661	0.336986	0.170254	0.113738	0.064266	0.336986	0.510763	0.568689	0.642661	0.488965	0.440050	0.448924
800	5.154800	No log	0.332681	0.509589	0.563601	0.650881	0.332681	0.169863	0.112720	0.065088	0.332681	0.509589	0.563601	0.650881	0.488864	0.437687	0.446900
899	5.154800	No log	0.345205	0.522896	0.581213	0.658708	0.345205	0.174299	0.116243	0.065871	0.345205	0.522896	0.581213	0.658708	0.500514	0.450292	0.459234
900	5.154800	No log	0.344423	0.523679	0.581605	0.656751	0.344423	0.174560	0.116321	0.065675	0.344423	0.523679	0.581605	0.656751	0.499776	0.449825	0.458906
1000	3.471200	No log	0.343249	0.523679	0.576125	0.649706	0.343249	0.174560	0.115225	0.064971	0.343249	0.523679	0.576125	0.649706	0.496883	0.448129	0.457042
1100	3.471200	No log	0.340117	0.515851	0.570646	0.641879	0.340117	0.171950	0.114129	0.064188	0.340117	0.515851	0.570646	0.641879	0.491739	0.443810	0.453156
1200	3.471200	No log	0.355382	0.531507	0.580431	0.651272	0.355382	0.177169	0.116086	0.065127	0.355382	0.531507	0.580431	0.651272	0.504323	0.457399	0.466116
1300	3.471200	No log	0.350294	0.524462	0.579256	0.643053	0.350294	0.174821	0.115851	0.064305	0.350294	0.524462	0.579256	0.643053	0.497413	0.450841	0.460497
1400	3.471200	No log	0.344814	0.516243	0.575734	0.645010	0.344814	0.172081	0.115147	0.064501	0.344814	0.516243	0.575734	0.645010	0.494792	0.446933	0.456397
1500	2.591000	No log	0.342074	0.521331	0.581605	0.649315	0.342074	0.173777	0.116321	0.064932	0.342074	0.521331	0.581605	0.649315	0.496009	0.446987	0.456283
1600	2.591000	No log	0.347945	0.527984	0.578082	0.650881	0.347945	0.175995	0.115616	0.065088	0.347945	0.527984	0.578082	0.650881	0.500572	0.452519	0.462155
1700	2.591000	No log	0.356947	0.532290	0.585127	0.659491	0.356947	0.177430	0.117025	0.065949	0.356947	0.532290	0.585127	0.659491	0.508829	0.460760	0.469761
1798	2.591000	No log	0.338943	0.523679	0.586301	0.667710	0.338943	0.174560	0.117260	0.066771	0.338943	0.523679	0.586301	0.667710	0.501051	0.448087	0.456467
1800	2.591000	No log	0.341683	0.520939	0.585519	0.668493	0.341683	0.173646	0.117104	0.066849	0.341683	0.520939	0.585519	0.668493	0.502045	0.449248	0.457581
1900	2.591000	No log	0.345988	0.535029	0.588650	0.655969	0.345988	0.178343	0.117730	0.065597	0.345988	0.535029	0.588650	0.655969	0.503222	0.454223	0.463424
2000	1.970200	No log	0.340900	0.533072	0.589824	0.659491	0.340900	0.177691	0.117965	0.065949	0.340900	0.533072	0.589824	0.659491	0.501478	0.450838	0.460119
2100	1.970200	No log	0.344423	0.527984	0.582779	0.658708	0.344423	0.175995	0.116556	0.065871	0.344423	0.527984	0.582779	0.658708	0.501615	0.451455	0.460175
2200	1.970200	No log	0.337769	0.526027	0.582387	0.655969	0.337769	0.175342	0.116477	0.065597	0.337769	0.526027	0.582387	0.655969	0.497888	0.447326	0.456343
2300	1.970200	No log	0.344031	0.527593	0.585519	0.654795	0.344031	0.175864	0.117104	0.065479	0.344031	0.527593	0.585519	0.654795	0.500674	0.451301	0.460238
2400	1.970200	No log	0.333464	0.520939	0.582779	0.657534	0.333464	0.173646	0.116556	0.065753	0.333464	0.520939	0.582779	0.657534	0.494823	0.442925	0.452111
2500	1.494300	No log	0.341683	0.521722	0.582779	0.657926	0.341683	0.173907	0.116556	0.065793	0.341683	0.521722	0.582779	0.657926	0.498909	0.448274	0.457438
2600	1.494300	No log	0.345597	0.533072	0.586301	0.661448	0.345597	0.177691	0.117260	0.066145	0.345597	0.533072	0.586301	0.661448	0.504436	0.454326	0.463081
2697	1.494300	No log	0.349119	0.534247	0.592172	0.664188	0.349119	0.178082	0.118434	0.066419	0.349119	0.534247	0.592172	0.664188	0.506395	0.456051	0.465072
2700	1.494300	No log	0.348337	0.534247	0.592564	0.663014	0.348337	0.178082	0.118513	0.066301	0.348337	0.534247	0.592564	0.663014	0.505657	0.455395	0.464421
2800	1.494300	No log	0.347945	0.534247	0.592564	0.663405	0.347945	0.178082	0.118513	0.066341	0.347945	0.534247	0.592564	0.663405	0.506186	0.455900	0.464603
2900	1.494300	No log	0.344814	0.533855	0.588258	0.657143	0.344814	0.177952	0.117652	0.065714	0.344814	0.533855	0.588258	0.657143	0.502024	0.452378	0.461676
3000	1.196000	No log	0.340509	0.531507	0.587476	0.657143	0.340509	0.177169	0.117495	0.065714	0.340509	0.531507	0.587476	0.657143	0.500715	0.450599	0.459781
3100	1.196000	No log	0.347162	0.529941	0.586301	0.660665	0.347162	0.176647	0.117260	0.066067	0.347162	0.529941	0.586301	0.660665	0.504210	0.454305	0.463088
3200	1.196000	No log	0.356556	0.524853	0.586301	0.657143	0.356556	0.174951	0.117260	0.065714	0.356556	0.524853	0.586301	0.657143	0.505897	0.457806	0.467231
3300	1.196000	No log	0.349511	0.533072	0.594129	0.663405	0.349511	0.177691	0.118826	0.066341	0.349511	0.533072	0.594129	0.663405	0.506837	0.456786	0.465756
3400	1.196000	No log	0.350294	0.527202	0.585127	0.667710	0.350294	0.175734	0.117025	0.066771	0.350294	0.527202	0.585127	0.667710	0.507453	0.456568	0.465428
3500	0.952400	No log	0.357730	0.537378	0.601174	0.667710	0.357730	0.179126	0.120235	0.066771	0.357730	0.537378	0.601174	0.667710	0.513222	0.463789	0.472255
3596	0.952400	No log	0.351859	0.530724	0.589041	0.666145	0.351859	0.176908	0.117808	0.066614	0.351859	0.530724	0.589041	0.666145	0.507550	0.457152	0.465970
3600	0.952400	No log	0.351076	0.527984	0.589432	0.665753	0.351076	0.175995	0.117886	0.066575	0.351076	0.527984	0.589432	0.665753	0.507098	0.456635	0.465430
3700	0.952400	No log	0.357730	0.539726	0.590998	0.663796	0.357730	0.179909	0.118200	0.066380	0.357730	0.539726	0.590998	0.663796	0.511145	0.462390	0.471283
3800	0.952400	No log	0.347162	0.531507	0.590215	0.666536	0.347162	0.177169	0.118043	0.066654	0.347162	0.531507	0.590215	0.666536	0.506289	0.455239	0.464315
3900	0.952400	No log	0.345988	0.529941	0.590607	0.663014	0.345988	0.176647	0.118121	0.066301	0.345988	0.529941	0.590607	0.663014	0.504410	0.453822	0.462890
4000	0.729800	No log	0.344423	0.530724	0.589824	0.659491	0.344423	0.176908	0.117965	0.065949	0.344423	0.530724	0.589824	0.659491	0.502288	0.452084	0.461532
4100	0.729800	No log	0.355773	0.529159	0.588650	0.658708	0.355773	0.176386	0.117730	0.065871	0.355773	0.529159	0.588650	0.658708	0.506950	0.458520	0.467878
4200	0.729800	No log	0.345597	0.529941	0.589041	0.660665	0.345597	0.176647	0.117808	0.066067	0.345597	0.529941	0.589041	0.660665	0.502855	0.452511	0.461609
4300	0.729800	No log	0.351468	0.532290	0.593738	0.672016	0.351468	0.177430	0.118748	0.067202	0.351468	0.532290	0.593738	0.672016	0.509763	0.458197	0.466783
4400	0.729800	No log	0.349119	0.528376	0.578474	0.650098	0.349119	0.176125	0.115695	0.065010	0.349119	0.528376	0.578474	0.650098	0.500010	0.452127	0.461150
4495	0.729800	No log	0.356164	0.530724	0.588258	0.662622	0.356164	0.176908	0.117652	0.066262	0.356164	0.530724	0.588258	0.662622	0.508426	0.459423	0.467895
4500	0.681800	No log	0.354990	0.528767	0.587867	0.664188	0.354990	0.176256	0.117573	0.066419	0.354990	0.528767	0.587867	0.664188	0.508228	0.458674	0.467256
4600	0.681800	No log	0.351076	0.529941	0.591389	0.662622	0.351076	0.176647	0.118278	0.066262	0.351076	0.529941	0.591389	0.662622	0.506293	0.456440	0.465229
4700	0.681800	No log	0.347945	0.519765	0.574560	0.647750	0.347945	0.173255	0.114912	0.064775	0.347945	0.519765	0.574560	0.647750	0.496778	0.448738	0.458103
4800	0.681800	No log	0.344423	0.526810	0.581996	0.659883	0.344423	0.175603	0.116399	0.065988	0.344423	0.526810	0.581996	0.659883	0.501660	0.451249	0.460354
4900	0.681800	No log	0.349511	0.525245	0.583562	0.661448	0.349511	0.175082	0.116712	0.066145	0.349511	0.525245	0.583562	0.661448	0.503672	0.453523	0.462625
5000	0.454900	No log	0.353425	0.531507	0.587867	0.665362	0.353425	0.177169	0.117573	0.066536	0.353425	0.531507	0.587867	0.665362	0.508258	0.458261	0.467302
5100	0.454900	No log	0.354207	0.536204	0.584344	0.659883	0.354207	0.178735	0.116869	0.065988	0.354207	0.536204	0.584344	0.659883	0.506794	0.458035	0.467649
5200	0.454900	No log	0.351076	0.536204	0.592564	0.668493	0.351076	0.178735	0.118513	0.066849	0.351076	0.536204	0.592564	0.668493	0.509546	0.458856	0.467915
5300	0.454900	No log	0.346380	0.535421	0.585127	0.657926	0.346380	0.178474	0.117025	0.065793	0.346380	0.535421	0.585127	0.657926	0.503709	0.454322	0.463912
5394	0.454900	No log	0.345597	0.539726	0.591781	0.661057	0.345597	0.179909	0.118356	0.066106	0.345597	0.539726	0.591781	0.661057	0.505330	0.455372	0.465167
5400	0.454900	No log	0.349902	0.541292	0.593738	0.660665	0.349902	0.180431	0.118748	0.066067	0.349902	0.541292	0.593738	0.660665	0.507531	0.458352	0.468122
5500	0.461500	No log	0.357339	0.539335	0.597652	0.666145	0.357339	0.179778	0.119530	0.066614	0.357339	0.539335	0.597652	0.666145	0.512167	0.462931	0.472007
5600	0.461500	No log	0.353033	0.540900	0.593738	0.666536	0.353033	0.180300	0.118748	0.066654	0.353033	0.540900	0.593738	0.666536	0.510756	0.460960	0.470240
5700	0.461500	No log	0.351468	0.538160	0.592955	0.665753	0.351468	0.179387	0.118591	0.066575	0.351468	0.538160	0.592955	0.665753	0.509360	0.459456	0.468414
5800	0.461500	No log	0.355773	0.539726	0.598434	0.666145	0.355773	0.179909	0.119687	0.066614	0.355773	0.539726	0.598434	0.666145	0.511199	0.461721	0.471114
5900	0.461500	No log	0.350685	0.540900	0.600391	0.667319	0.350685	0.180300	0.120078	0.066732	0.350685	0.540900	0.600391	0.667319	0.510758	0.460510	0.469823
6000	0.397000	No log	0.353425	0.538552	0.600000	0.671233	0.353425	0.179517	0.120000	0.067123	0.353425	0.538552	0.600000	0.671233	0.511820	0.460974	0.470030
6100	0.397000	No log	0.356164	0.537769	0.595303	0.671233	0.356164	0.179256	0.119061	0.067123	0.356164	0.537769	0.595303	0.671233	0.512703	0.462191	0.471077
6200	0.397000	No log	0.356947	0.547554	0.601957	0.667710	0.356947	0.182518	0.120391	0.066771	0.356947	0.547554	0.601957	0.667710	0.513127	0.463591	0.473154
6293	0.397000	No log	0.358904	0.540117	0.600783	0.667319	0.358904	0.180039	0.120157	0.066732	0.358904	0.540117	0.600783	0.667319	0.513301	0.464049	0.473172
6300	0.397000	No log	0.358121	0.537378	0.600391	0.665362	0.358121	0.179126	0.120078	0.066536	0.358121	0.537378	0.600391	0.665362	0.511845	0.462741	0.472018
6400	0.397000	No log	0.358904	0.544423	0.594912	0.662622	0.358904	0.181474	0.118982	0.066262	0.358904	0.544423	0.594912	0.662622	0.512283	0.464115	0.473868
6500	0.324000	No log	0.354990	0.541292	0.600000	0.670841	0.354990	0.180431	0.120000	0.067084	0.354990	0.541292	0.600000	0.670841	0.513000	0.462599	0.471705
6600	0.324000	No log	0.353425	0.545597	0.599609	0.673190	0.353425	0.181866	0.119922	0.067319	0.353425	0.545597	0.599609	0.673190	0.513679	0.462763	0.471442
6700	0.324000	No log	0.357339	0.545597	0.603131	0.671624	0.357339	0.181866	0.120626	0.067162	0.357339	0.545597	0.603131	0.671624	0.515639	0.465680	0.474605
6800	0.324000	No log	0.351076	0.547162	0.602348	0.675930	0.351076	0.182387	0.120470	0.067593	0.351076	0.547162	0.602348	0.675930	0.514280	0.462586	0.471418
6900	0.324000	No log	0.356556	0.547945	0.603523	0.672016	0.356556	0.182648	0.120705	0.067202	0.356556	0.547945	0.603523	0.672016	0.515773	0.465697	0.474727
7000	0.304500	No log	0.361644	0.552250	0.609785	0.678278	0.361644	0.184083	0.121957	0.067828	0.361644	0.552250	0.609785	0.678278	0.520728	0.470309	0.478842
7100	0.304500	No log	0.359295	0.545597	0.603131	0.673581	0.359295	0.181866	0.120626	0.067358	0.359295	0.545597	0.603131	0.673581	0.516244	0.466059	0.475160
7192	0.304500	No log	0.356556	0.544423	0.600783	0.673973	0.356556	0.181474	0.120157	0.067397	0.356556	0.544423	0.600783	0.673973	0.515351	0.464718	0.473690
7200	0.304500	No log	0.357339	0.544031	0.601957	0.674364	0.357339	0.181344	0.120391	0.067436	0.357339	0.544031	0.601957	0.674364	0.515852	0.465255	0.474194
7300	0.304500	No log	0.356947	0.544423	0.599609	0.672016	0.356947	0.181474	0.119922	0.067202	0.356947	0.544423	0.599609	0.672016	0.515550	0.465505	0.474351
7400	0.304500	No log	0.359687	0.545205	0.601174	0.667319	0.359687	0.181735	0.120235	0.066732	0.359687	0.545205	0.601174	0.667319	0.515192	0.466433	0.475604
7500	0.239600	No log	0.362818	0.544423	0.599217	0.663014	0.362818	0.181474	0.119843	0.066301	0.362818	0.544423	0.599217	0.663014	0.514694	0.467116	0.476696
7600	0.239600	No log	0.358904	0.546771	0.594912	0.668493	0.358904	0.182257	0.118982	0.066849	0.358904	0.546771	0.594912	0.668493	0.514987	0.465891	0.475035
7700	0.239600	No log	0.358904	0.546771	0.601174	0.671233	0.358904	0.182257	0.120235	0.067123	0.358904	0.546771	0.601174	0.671233	0.515977	0.466329	0.475209
7800	0.239600	No log	0.362035	0.549902	0.599217	0.668885	0.362035	0.183301	0.119843	0.066888	0.362035	0.549902	0.599217	0.668885	0.516800	0.468085	0.477242
7900	0.239600	No log	0.358513	0.548728	0.600783	0.666928	0.358513	0.182909	0.120157	0.066693	0.358513	0.548728	0.600783	0.666928	0.514917	0.466078	0.475310
8000	0.254800	No log	0.357339	0.548728	0.602348	0.668885	0.357339	0.182909	0.120470	0.066888	0.357339	0.548728	0.602348	0.668885	0.515002	0.465673	0.474771
8091	0.254800	No log	0.357730	0.547554	0.602740	0.668493	0.357730	0.182518	0.120548	0.066849	0.357730	0.547554	0.602740	0.668493	0.514967	0.465726	0.474765
8100	0.254800	No log	0.356947	0.548728	0.603131	0.667319	0.356947	0.182909	0.120626	0.066732	0.356947	0.548728	0.603131	0.667319	0.514231	0.465074	0.474208
8200	0.254800	No log	0.357339	0.546771	0.602348	0.670841	0.357339	0.182257	0.120470	0.067084	0.357339	0.546771	0.602348	0.670841	0.515365	0.465572	0.474389
8300	0.254800	No log	0.356947	0.549902	0.603914	0.670059	0.356947	0.183301	0.120783	0.067006	0.356947	0.549902	0.603914	0.670059	0.515590	0.466024	0.474931
8400	0.254800	No log	0.357339	0.550294	0.605088	0.669276	0.357339	0.183431	0.121018	0.066928	0.357339	0.550294	0.605088	0.669276	0.515114	0.465636	0.474681
8500	0.212100	No log	0.358513	0.547945	0.601957	0.670450	0.358513	0.182648	0.120391	0.067045	0.358513	0.547945	0.601957	0.670450	0.515278	0.465638	0.474501
8600	0.212100	No log	0.358121	0.550685	0.603523	0.668885	0.358121	0.183562	0.120705	0.066888	0.358121	0.550685	0.603523	0.668885	0.515006	0.465618	0.474601
8700	0.212100	No log	0.358904	0.547945	0.603914	0.668102	0.358904	0.182648	0.120783	0.066810	0.358904	0.547945	0.603914	0.668102	0.515153	0.466067	0.475061
8800	0.212100	No log	0.358513	0.549511	0.603131	0.668102	0.358513	0.183170	0.120626	0.066810	0.358513	0.549511	0.603131	0.668102	0.514955	0.465827	0.474854
8900	0.212100	No log	0.358904	0.549902	0.603131	0.667710	0.358904	0.183301	0.120626	0.066771	0.358904	0.549902	0.603131	0.667710	0.514998	0.466000	0.475080
8990	0.212100	No log	0.358904	0.549511	0.603131	0.668102	0.358904	0.183170	0.120626	0.066810	0.358904	0.549511	0.603131	0.668102	0.515068	0.465991	0.475023





EPOCHS = 10
BATCH_SIZE = 16
LEARNING_RATE =2e-5
outpath = 'finetuned_tdb_paraphrase-multilingual_mpnet_try00'


loader = DataLoader(
    examples, batch_size=BATCH_SIZE
)
WARMUP_STEPS = int(len(loader) * EPOCHS * 0.1)


model.fit(
    train_objectives=[(loader, train_loss)],
    epochs=EPOCHS,
    warmup_steps=WARMUP_STEPS,
    output_path=outpath,
    show_progress_bar=True,
    evaluator=evaluator,
    evaluation_steps=100,
    optimizer_params={"lr": LEARNING_RATE},
)


—--------------------------------------------
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)
wandb: You can find your API key in your browser here: https://wandb.ai/authorize
wandb: Paste an API key from your profile and hit enter: ··········
wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.
wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.
wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc
wandb: Currently logged in as: gmunkhtur to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
Tracking run with wandb version 0.19.6
Run data is saved locally in /content/wandb/run-20250213_073947-wtp6f2av
Syncing run checkpoints/model to Weights & Biases (docs)
View project at https://wandb.ai/gmunkhtur/sentence-transformers
View run at https://wandb.ai/gmunkhtur/sentence-transformers/runs/wtp6f2av
 [4620/4620 1:00:17, Epoch 10/10]
Step	Training Loss	Validation Loss	Cosine Accuracy@1	Cosine Accuracy@3	Cosine Accuracy@5	Cosine Accuracy@10	Cosine Precision@1	Cosine Precision@3	Cosine Precision@5	Cosine Precision@10	Cosine Recall@1	Cosine Recall@3	Cosine Recall@5	Cosine Recall@10	Cosine Ndcg@10	Cosine Mrr@10	Cosine Map@100
100	No log	No log	0.204336	0.337127	0.399458	0.501355	0.204336	0.112376	0.079892	0.050136	0.204336	0.337127	0.399458	0.501355	0.340417	0.290426	0.303459
200	No log	No log	0.249864	0.416802	0.491057	0.597290	0.249864	0.138934	0.098211	0.059729	0.249864	0.416802	0.491057	0.597290	0.411353	0.353412	0.365521
300	No log	No log	0.282385	0.452575	0.533875	0.635230	0.282385	0.150858	0.106775	0.063523	0.282385	0.452575	0.533875	0.635230	0.447803	0.389085	0.401562
400	No log	No log	0.281843	0.483469	0.563686	0.675339	0.281843	0.161156	0.112737	0.067534	0.281843	0.483469	0.563686	0.675339	0.469395	0.404740	0.416229
462	No log	No log	0.302981	0.488889	0.583740	0.680759	0.302981	0.162963	0.116748	0.068076	0.302981	0.488889	0.583740	0.680759	0.482845	0.420422	0.431821
500	4.642300	No log	0.300271	0.498645	0.578320	0.697561	0.300271	0.166215	0.115664	0.069756	0.300271	0.498645	0.578320	0.697561	0.488073	0.422498	0.433654
600	4.642300	No log	0.311111	0.510569	0.601084	0.703523	0.311111	0.170190	0.120217	0.070352	0.311111	0.510569	0.601084	0.703523	0.497685	0.432883	0.444523
700	4.642300	No log	0.313821	0.514363	0.607046	0.717615	0.313821	0.171454	0.121409	0.071762	0.313821	0.514363	0.607046	0.717615	0.505162	0.438417	0.448830
800	4.642300	No log	0.319783	0.527371	0.604878	0.721951	0.319783	0.175790	0.120976	0.072195	0.319783	0.527371	0.604878	0.721951	0.512082	0.446012	0.456610
900	4.642300	No log	0.320867	0.546341	0.639566	0.754472	0.320867	0.182114	0.127913	0.075447	0.320867	0.546341	0.639566	0.754472	0.528229	0.456888	0.466197
924	4.642300	No log	0.330081	0.551762	0.639566	0.753930	0.330081	0.183921	0.127913	0.075393	0.330081	0.551762	0.639566	0.753930	0.533316	0.463674	0.473020
1000	2.058000	No log	0.333333	0.552304	0.633604	0.740379	0.333333	0.184101	0.126721	0.074038	0.333333	0.552304	0.633604	0.740379	0.530158	0.463765	0.473802
1100	2.058000	No log	0.346883	0.564770	0.657453	0.757182	0.346883	0.188257	0.131491	0.075718	0.346883	0.564770	0.657453	0.757182	0.545138	0.478013	0.488063
1200	2.058000	No log	0.346341	0.569648	0.662331	0.760434	0.346341	0.189883	0.132466	0.076043	0.346341	0.569648	0.662331	0.760434	0.547352	0.479830	0.489668
1300	2.058000	No log	0.351220	0.574526	0.669919	0.778862	0.351220	0.191509	0.133984	0.077886	0.351220	0.574526	0.669919	0.778862	0.557171	0.487147	0.496149
1386	2.058000	No log	0.347425	0.583198	0.673713	0.778862	0.347425	0.194399	0.134743	0.077886	0.347425	0.583198	0.673713	0.778862	0.555709	0.485071	0.494100
1400	2.058000	No log	0.360434	0.588618	0.673171	0.778862	0.360434	0.196206	0.134634	0.077886	0.360434	0.588618	0.673171	0.778862	0.562487	0.494025	0.503245
1500	1.252700	No log	0.366938	0.592954	0.678591	0.783198	0.366938	0.197651	0.135718	0.078320	0.366938	0.592954	0.678591	0.783198	0.568003	0.499932	0.508631
1600	1.252700	No log	0.370732	0.595664	0.686179	0.784282	0.370732	0.198555	0.137236	0.078428	0.370732	0.595664	0.686179	0.784282	0.571275	0.503809	0.512855
1700	1.252700	No log	0.372900	0.603252	0.688347	0.782114	0.372900	0.201084	0.137669	0.078211	0.372900	0.603252	0.688347	0.782114	0.572441	0.505856	0.514848
1800	1.252700	No log	0.365854	0.610298	0.698103	0.793496	0.365854	0.203433	0.139621	0.079350	0.365854	0.610298	0.698103	0.793496	0.577142	0.508089	0.516608
1848	1.252700	No log	0.363686	0.613008	0.701355	0.806504	0.363686	0.204336	0.140271	0.080650	0.363686	0.613008	0.701355	0.806504	0.579892	0.507909	0.516218
1900	1.252700	No log	0.367480	0.597832	0.697561	0.802168	0.367480	0.199277	0.139512	0.080217	0.367480	0.597832	0.697561	0.802168	0.578082	0.507064	0.515438
2000	0.898600	No log	0.377236	0.609214	0.697019	0.799458	0.377236	0.203071	0.139404	0.079946	0.377236	0.609214	0.697019	0.799458	0.581678	0.512603	0.521050
2100	0.898600	No log	0.371274	0.617886	0.707859	0.806504	0.371274	0.205962	0.141572	0.080650	0.371274	0.617886	0.707859	0.806504	0.585155	0.514609	0.522697
2200	0.898600	No log	0.377236	0.620054	0.707859	0.811382	0.377236	0.206685	0.141572	0.081138	0.377236	0.620054	0.707859	0.811382	0.589614	0.519158	0.526924
2300	0.898600	No log	0.376694	0.617344	0.702981	0.809214	0.376694	0.205781	0.140596	0.080921	0.376694	0.617344	0.702981	0.809214	0.589241	0.519288	0.527523
2310	0.898600	No log	0.378862	0.622222	0.705149	0.808130	0.378862	0.207407	0.141030	0.080813	0.378862	0.622222	0.705149	0.808130	0.590554	0.521281	0.529496
2400	0.898600	No log	0.372358	0.623848	0.710569	0.816802	0.372358	0.207949	0.142114	0.081680	0.372358	0.623848	0.710569	0.816802	0.590817	0.518843	0.526626
2500	0.607000	No log	0.378862	0.617886	0.706233	0.818428	0.378862	0.205962	0.141247	0.081843	0.378862	0.617886	0.706233	0.818428	0.593158	0.521701	0.529397
2600	0.607000	No log	0.392954	0.627100	0.717615	0.816260	0.392954	0.209033	0.143523	0.081626	0.392954	0.627100	0.717615	0.816260	0.600332	0.531616	0.539244
2700	0.607000	No log	0.391328	0.633062	0.714905	0.824390	0.391328	0.211021	0.142981	0.082439	0.391328	0.633062	0.714905	0.824390	0.601376	0.530669	0.538071
2772	0.607000	No log	0.394580	0.637940	0.722493	0.829810	0.394580	0.212647	0.144499	0.082981	0.394580	0.637940	0.722493	0.829810	0.606318	0.535440	0.542441
2800	0.607000	No log	0.386450	0.633062	0.718699	0.826016	0.386450	0.211021	0.143740	0.082602	0.386450	0.633062	0.718699	0.826016	0.601734	0.530441	0.537590
2900	0.607000	No log	0.393496	0.644986	0.726829	0.824390	0.393496	0.214995	0.145366	0.082439	0.393496	0.644986	0.726829	0.824390	0.605822	0.536159	0.543591
3000	0.539400	No log	0.390244	0.642818	0.718699	0.828184	0.390244	0.214273	0.143740	0.082818	0.390244	0.642818	0.718699	0.828184	0.604442	0.533368	0.540595
3100	0.539400	No log	0.398374	0.642818	0.724661	0.827100	0.398374	0.214273	0.144932	0.082710	0.398374	0.642818	0.724661	0.827100	0.609305	0.539985	0.547387
3200	0.539400	No log	0.386992	0.639024	0.723035	0.829268	0.386992	0.213008	0.144607	0.082927	0.386992	0.639024	0.723035	0.829268	0.603894	0.532241	0.539510
3234	0.539400	No log	0.393496	0.641192	0.730623	0.829810	0.393496	0.213731	0.146125	0.082981	0.393496	0.641192	0.730623	0.829810	0.607278	0.536441	0.543666
3300	0.539400	No log	0.391870	0.646612	0.726829	0.835230	0.391870	0.215537	0.145366	0.083523	0.391870	0.646612	0.726829	0.835230	0.609271	0.537464	0.544375
3400	0.539400	No log	0.399458	0.646070	0.730081	0.840650	0.399458	0.215357	0.146016	0.084065	0.399458	0.646070	0.730081	0.840650	0.613931	0.542046	0.548757
3500	0.447200	No log	0.398916	0.650407	0.738753	0.841734	0.398916	0.216802	0.147751	0.084173	0.398916	0.650407	0.738753	0.841734	0.616630	0.544914	0.551361
3600	0.447200	No log	0.402168	0.650949	0.744173	0.839566	0.402168	0.216983	0.148835	0.083957	0.402168	0.650949	0.744173	0.839566	0.618217	0.547606	0.554534
3696	0.447200	No log	0.409756	0.660705	0.744173	0.839024	0.409756	0.220235	0.148835	0.083902	0.409756	0.660705	0.744173	0.839024	0.622090	0.552795	0.559685
3700	0.447200	No log	0.410298	0.661247	0.745799	0.837940	0.410298	0.220416	0.149160	0.083794	0.410298	0.661247	0.745799	0.837940	0.621938	0.552909	0.559876
3800	0.447200	No log	0.404878	0.663957	0.746341	0.839024	0.404878	0.221319	0.149268	0.083902	0.404878	0.663957	0.746341	0.839024	0.620822	0.551006	0.557861
3900	0.447200	No log	0.408672	0.662331	0.736043	0.840650	0.408672	0.220777	0.147209	0.084065	0.408672	0.662331	0.736043	0.840650	0.621611	0.551755	0.558505
4000	0.329200	No log	0.406504	0.662331	0.742005	0.848238	0.406504	0.220777	0.148401	0.084824	0.406504	0.662331	0.742005	0.848238	0.624553	0.553333	0.559571
4100	0.329200	No log	0.407046	0.663415	0.749051	0.844986	0.407046	0.221138	0.149810	0.084499	0.407046	0.663415	0.749051	0.844986	0.624280	0.553815	0.560344
4158	0.329200	No log	0.410840	0.663957	0.745799	0.843902	0.410840	0.221319	0.149160	0.084390	0.410840	0.663957	0.745799	0.843902	0.625610	0.555858	0.562590
4200	0.329200	No log	0.408130	0.663957	0.745257	0.847154	0.408130	0.221319	0.149051	0.084715	0.408130	0.663957	0.745257	0.847154	0.625228	0.554465	0.560837
4300	0.329200	No log	0.409756	0.668293	0.747967	0.843902	0.409756	0.222764	0.149593	0.084390	0.409756	0.668293	0.747967	0.843902	0.625500	0.555644	0.562335
4400	0.329200	No log	0.408130	0.666125	0.749593	0.846070	0.408130	0.222042	0.149919	0.084607	0.408130	0.666125	0.749593	0.846070	0.625737	0.555249	0.561843
4500	0.333800	No log	0.410298	0.669919	0.749593	0.844444	0.410298	0.223306	0.149919	0.084444	0.410298	0.669919	0.749593	0.844444	0.626406	0.556583	0.563290
4600	0.333800	No log	0.411382	0.668835	0.750678	0.844986	0.411382	0.222945	0.150136	0.084499	0.411382	0.668835	0.750678	0.844986	0.627147	0.557424	0.564070
4620	0.333800	No log	0.411382	0.668835	0.750678	0.844986	0.411382	0.222945	0.150136	0.084499	0.411382	0.668835	0.750678	0.844986	0.627147	0.557424	0.564070




EPOCHS =6
BATCH_SIZE = 16
LEARNING_RATE =1e-5
outpath = 'finetuned_tdb_paraphrase-multilingual_mpnet_try1'


loader = DataLoader(
    examples, batch_size=BATCH_SIZE
)
WARMUP_STEPS = int(len(loader) * EPOCHS * 0.15)


model.fit(
    train_objectives=[(loader, train_loss)],
    epochs=EPOCHS,
    warmup_steps=WARMUP_STEPS,
    output_path=outpath,
    show_progress_bar=True,
    evaluator=evaluator,
    weight_decay=0.05,
    max_grad_norm=0.5,
    evaluation_steps=100,


    optimizer_params={"lr": LEARNING_RATE},
)


[2772/2772 33:55, Epoch 6/6]
Step	Training Loss	Validation Loss	Cosine Accuracy@1	Cosine Accuracy@3	Cosine Accuracy@5	Cosine Accuracy@10	Cosine Precision@1	Cosine Precision@3	Cosine Precision@5	Cosine Precision@10	Cosine Recall@1	Cosine Recall@3	Cosine Recall@5	Cosine Recall@10	Cosine Ndcg@10	Cosine Mrr@10	Cosine Map@100
100	No log	No log	0.411924	0.663957	0.751220	0.846070	0.411924	0.221319	0.150244	0.084607	0.411924	0.663957	0.751220	0.846070	0.626481	0.556334	0.562846
200	No log	No log	0.414092	0.661789	0.748509	0.846070	0.414092	0.220596	0.149702	0.084607	0.414092	0.661789	0.748509	0.846070	0.627577	0.557817	0.564289
300	No log	No log	0.411924	0.652575	0.744715	0.836856	0.411924	0.217525	0.148943	0.083686	0.411924	0.652575	0.744715	0.836856	0.620928	0.552114	0.559041
400	No log	No log	0.412466	0.654743	0.744173	0.837940	0.412466	0.218248	0.148835	0.083794	0.412466	0.654743	0.744173	0.837940	0.622761	0.554152	0.561305
462	No log	No log	0.407046	0.655285	0.745257	0.840650	0.407046	0.218428	0.149051	0.084065	0.407046	0.655285	0.745257	0.840650	0.621964	0.552188	0.558761
500	0.277600	No log	0.407046	0.655285	0.743089	0.836314	0.407046	0.218428	0.148618	0.083631	0.407046	0.655285	0.743089	0.836314	0.619215	0.549950	0.556896
600	0.277600	No log	0.410298	0.653117	0.737669	0.835230	0.410298	0.217706	0.147534	0.083523	0.410298	0.653117	0.737669	0.835230	0.620663	0.552220	0.559125
700	0.277600	No log	0.407046	0.653117	0.742547	0.836314	0.407046	0.217706	0.148509	0.083631	0.407046	0.653117	0.742547	0.836314	0.617620	0.547855	0.554558
800	0.277600	No log	0.410840	0.657453	0.743631	0.833062	0.410840	0.219151	0.148726	0.083306	0.410840	0.657453	0.743631	0.833062	0.619439	0.551168	0.558134
900	0.277600	No log	0.416802	0.661247	0.747967	0.837940	0.416802	0.220416	0.149593	0.083794	0.416802	0.661247	0.747967	0.837940	0.624295	0.556170	0.562706
924	0.277600	No log	0.414634	0.664499	0.747967	0.836314	0.414634	0.221500	0.149593	0.083631	0.414634	0.664499	0.747967	0.836314	0.624002	0.556118	0.562859
1000	0.204600	No log	0.408672	0.655285	0.741463	0.836314	0.408672	0.218428	0.148293	0.083631	0.408672	0.655285	0.741463	0.836314	0.620077	0.551051	0.557848
1100	0.204600	No log	0.414634	0.660705	0.750136	0.840650	0.414634	0.220235	0.150027	0.084065	0.414634	0.660705	0.750136	0.840650	0.626050	0.557469	0.564132
1200	0.204600	No log	0.410840	0.663415	0.745257	0.836314	0.410840	0.221138	0.149051	0.083631	0.410840	0.663415	0.745257	0.836314	0.622540	0.554083	0.561001
1300	0.204600	No log	0.418970	0.663415	0.753388	0.835230	0.418970	0.221138	0.150678	0.083523	0.418970	0.663415	0.753388	0.835230	0.626770	0.559886	0.566905
1386	0.204600	No log	0.410298	0.668293	0.749051	0.839024	0.410298	0.222764	0.149810	0.083902	0.410298	0.668293	0.749051	0.839024	0.623973	0.555078	0.561935
1400	0.204600	No log	0.410298	0.666667	0.751220	0.839024	0.410298	0.222222	0.150244	0.083902	0.410298	0.666667	0.751220	0.839024	0.623293	0.554253	0.561083
1500	0.165600	No log	0.414092	0.663957	0.753930	0.834688	0.414092	0.221319	0.150786	0.083469	0.414092	0.663957	0.753930	0.834688	0.624021	0.556403	0.563370
1600	0.165600	No log	0.409756	0.665583	0.754472	0.839024	0.409756	0.221861	0.150894	0.083902	0.409756	0.665583	0.754472	0.839024	0.624763	0.555918	0.562565
1700	0.165600	No log	0.418970	0.665583	0.752304	0.837940	0.418970	0.221861	0.150461	0.083794	0.418970	0.665583	0.752304	0.837940	0.627272	0.559751	0.566414
1800	0.165600	No log	0.411382	0.665041	0.756098	0.837940	0.411382	0.221680	0.151220	0.083794	0.411382	0.665041	0.756098	0.837940	0.624339	0.555789	0.562584
1848	0.165600	No log	0.421138	0.663957	0.756640	0.839024	0.421138	0.221319	0.151328	0.083902	0.421138	0.663957	0.756640	0.839024	0.629199	0.561948	0.568817
1900	0.165600	No log	0.417344	0.665583	0.753930	0.839024	0.417344	0.221861	0.150786	0.083902	0.417344	0.665583	0.753930	0.839024	0.627310	0.559399	0.566288
2000	0.123900	No log	0.415176	0.673171	0.759892	0.844444	0.415176	0.224390	0.151978	0.084444	0.415176	0.673171	0.759892	0.844444	0.628726	0.559543	0.565878
2100	0.123900	No log	0.405962	0.664499	0.760434	0.844986	0.405962	0.221500	0.152087	0.084499	0.405962	0.664499	0.760434	0.844986	0.625816	0.555436	0.561796
2200	0.123900	No log	0.407046	0.670461	0.759350	0.842818	0.407046	0.223487	0.151870	0.084282	0.407046	0.670461	0.759350	0.842818	0.626127	0.556394	0.563058
2300	0.123900	No log	0.409756	0.674255	0.758266	0.844986	0.409756	0.224752	0.151653	0.084499	0.409756	0.674255	0.758266	0.844986	0.627530	0.557723	0.564221
2310	0.123900	No log	0.408672	0.671545	0.757724	0.844444	0.408672	0.223848	0.151545	0.084444	0.408672	0.671545	0.757724	0.844444	0.626516	0.556560	0.563114
2400	0.123900	No log	0.408672	0.668293	0.756098	0.843902	0.408672	0.222764	0.151220	0.084390	0.408672	0.668293	0.756098	0.843902	0.626253	0.556374	0.562828
2500	0.091000	No log	0.411382	0.671003	0.759350	0.845528	0.411382	0.223668	0.151870	0.084553	0.411382	0.671003	0.759350	0.845528	0.628175	0.558423	0.564736
2600	0.091000	No log	0.411382	0.669919	0.757182	0.845528	0.411382	0.223306	0.151436	0.084553	0.411382	0.669919	0.757182	0.845528	0.628301	0.558602	0.565000
2700	0.091000	No log	0.411924	0.669919	0.758808	0.845528	0.411924	0.223306	0.151762	0.084553	0.411924	0.669919	0.758808	0.845528	0.628732	0.559164	0.565574
2772	0.091000	No log	0.411382	0.669919	0.758266	0.846070	0.411382	0.223306	0.151653	0.084607	0.411382	0.669919	0.758266	0.846070	0.628557	0.558791	0.565170



EPOCHS = 30
BATCH_SIZE = 16
LEARNING_RATE =2e-5
outpath = 'finetuned_tdb_paraphrase-multilingual_mpnet_try2'


loader = DataLoader(
    examples, batch_size=BATCH_SIZE
)
WARMUP_STEPS = int(len(loader) * EPOCHS * 0.1)


model.fit(
    train_objectives=[(loader, train_loss)],
    epochs=EPOCHS,
    warmup_steps=WARMUP_STEPS,
    output_path=outpath,
    show_progress_bar=True,
    evaluator=evaluator,
    evaluation_steps=100,
    optimizer_params={"lr": LEARNING_RATE},
)




wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)
wandb: You can find your API key in your browser here: https://wandb.ai/authorize
wandb: Paste an API key from your profile and hit enter: ··········
wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.
wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.
wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc
wandb: Currently logged in as: gmunkhtur to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
Tracking run with wandb version 0.19.6
Run data is saved locally in /content/wandb/run-20250215_104058-85piu00q
Syncing run checkpoints/model to Weights & Biases (docs)
View project at https://wandb.ai/gmunkhtur/sentence-transformers
View run at https://wandb.ai/gmunkhtur/sentence-transformers/runs/85piu00q
 [13860/13860 2:55:57, Epoch 30/30]
Step	Training Loss	Validation Loss	Cosine Accuracy@1	Cosine Accuracy@3	Cosine Accuracy@5	Cosine Accuracy@10	Cosine Precision@1	Cosine Precision@3	Cosine Precision@5	Cosine Precision@10	Cosine Recall@1	Cosine Recall@3	Cosine Recall@5	Cosine Recall@10	Cosine Ndcg@10	Cosine Mrr@10	Cosine Map@100
100	No log	No log	0.165312	0.286721	0.344715	0.437940	0.165312	0.095574	0.068943	0.043794	0.165312	0.286721	0.344715	0.437940	0.288391	0.242135	0.254470
200	No log	No log	0.208130	0.350136	0.416802	0.523577	0.208130	0.116712	0.083360	0.052358	0.208130	0.350136	0.416802	0.523577	0.352613	0.299563	0.312953
300	No log	No log	0.234146	0.398374	0.471545	0.576152	0.234146	0.132791	0.094309	0.057615	0.234146	0.398374	0.471545	0.576152	0.394451	0.337735	0.350208
400	No log	No log	0.253659	0.434146	0.510569	0.615176	0.253659	0.144715	0.102114	0.061518	0.253659	0.434146	0.510569	0.615176	0.424163	0.364237	0.376929
462	No log	No log	0.276423	0.460163	0.539837	0.638482	0.276423	0.153388	0.107967	0.063848	0.276423	0.460163	0.539837	0.638482	0.449303	0.389774	0.401414
500	5.606700	No log	0.275339	0.471003	0.552304	0.646612	0.275339	0.157001	0.110461	0.064661	0.275339	0.471003	0.552304	0.646612	0.453746	0.392974	0.404764
600	5.606700	No log	0.300271	0.493767	0.571274	0.666125	0.300271	0.164589	0.114255	0.066612	0.300271	0.493767	0.571274	0.666125	0.475576	0.415548	0.427378
700	5.606700	No log	0.303523	0.493225	0.581572	0.689973	0.303523	0.164408	0.116314	0.068997	0.303523	0.493225	0.581572	0.689973	0.486994	0.423331	0.434347
800	5.606700	No log	0.321409	0.513279	0.590244	0.688889	0.321409	0.171093	0.118049	0.068889	0.321409	0.513279	0.590244	0.688889	0.497425	0.437163	0.448310
900	5.606700	No log	0.317073	0.512195	0.608130	0.719241	0.317073	0.170732	0.121626	0.071924	0.317073	0.512195	0.608130	0.719241	0.507498	0.440953	0.451570
924	5.606700	No log	0.320325	0.521409	0.601084	0.711111	0.320325	0.173803	0.120217	0.071111	0.320325	0.521409	0.601084	0.711111	0.508263	0.444309	0.455832
1000	2.606900	No log	0.326287	0.531165	0.621680	0.712737	0.326287	0.177055	0.124336	0.071274	0.326287	0.531165	0.621680	0.712737	0.513110	0.449915	0.460510
1100	2.606900	No log	0.324661	0.550678	0.629268	0.732791	0.324661	0.183559	0.125854	0.073279	0.324661	0.550678	0.629268	0.732791	0.523178	0.456753	0.467516
1200	2.606900	No log	0.328997	0.545799	0.627642	0.737669	0.328997	0.181933	0.125528	0.073767	0.328997	0.545799	0.627642	0.737669	0.525790	0.458896	0.469418
1300	2.606900	No log	0.342005	0.555014	0.640650	0.753388	0.342005	0.185005	0.128130	0.075339	0.342005	0.555014	0.640650	0.753388	0.539256	0.471772	0.482242
1386	2.606900	No log	0.323035	0.546883	0.633062	0.747967	0.323035	0.182294	0.126612	0.074797	0.323035	0.546883	0.633062	0.747967	0.526306	0.456518	0.466416
1400	2.606900	No log	0.335501	0.549593	0.641192	0.752846	0.335501	0.183198	0.128238	0.075285	0.335501	0.549593	0.641192	0.752846	0.533896	0.464954	0.474678
1500	1.758800	No log	0.343631	0.576152	0.672087	0.768564	0.343631	0.192051	0.134417	0.076856	0.343631	0.576152	0.672087	0.768564	0.549488	0.480010	0.488801
1600	1.758800	No log	0.347425	0.574526	0.662331	0.769648	0.347425	0.191509	0.132466	0.076965	0.347425	0.574526	0.662331	0.769648	0.550872	0.481670	0.490900
1700	1.758800	No log	0.355014	0.569648	0.659621	0.750136	0.355014	0.189883	0.131924	0.075014	0.355014	0.569648	0.659621	0.750136	0.547711	0.483471	0.493638
1800	1.758800	No log	0.359892	0.581030	0.667751	0.775068	0.359892	0.193677	0.133550	0.077507	0.359892	0.581030	0.667751	0.775068	0.559518	0.491455	0.499963
1848	1.758800	No log	0.355556	0.584282	0.681843	0.797290	0.355556	0.194761	0.136369	0.079729	0.355556	0.584282	0.681843	0.797290	0.565913	0.493026	0.501117
1900	1.758800	No log	0.363144	0.580488	0.692141	0.802168	0.363144	0.193496	0.138428	0.080217	0.363144	0.580488	0.692141	0.802168	0.571402	0.498784	0.506457
2000	1.266500	No log	0.350678	0.585366	0.686721	0.780488	0.350678	0.195122	0.137344	0.078049	0.350678	0.585366	0.686721	0.780488	0.559941	0.489772	0.498771
2100	1.266500	No log	0.346341	0.589160	0.686721	0.789702	0.346341	0.196387	0.137344	0.078970	0.346341	0.589160	0.686721	0.789702	0.561731	0.489329	0.497668
2200	1.266500	No log	0.358266	0.591870	0.687263	0.784282	0.358266	0.197290	0.137453	0.078428	0.358266	0.591870	0.687263	0.784282	0.565571	0.496039	0.504848
2300	1.266500	No log	0.356098	0.592412	0.687263	0.794038	0.356098	0.197471	0.137453	0.079404	0.356098	0.592412	0.687263	0.794038	0.568138	0.496515	0.504904
2310	1.266500	No log	0.355556	0.592954	0.685637	0.792954	0.355556	0.197651	0.137127	0.079295	0.355556	0.592954	0.685637	0.792954	0.567103	0.495450	0.503860
2400	1.266500	No log	0.368022	0.601084	0.688889	0.795664	0.368022	0.200361	0.137778	0.079566	0.368022	0.601084	0.688889	0.795664	0.575214	0.505380	0.513679
2500	0.841800	No log	0.360976	0.604336	0.689973	0.800000	0.360976	0.201445	0.137995	0.080000	0.360976	0.604336	0.689973	0.800000	0.575289	0.503912	0.512128
2600	0.841800	No log	0.372358	0.609756	0.695935	0.795122	0.372358	0.203252	0.139187	0.079512	0.372358	0.609756	0.695935	0.795122	0.578697	0.509925	0.518217
2700	0.841800	No log	0.351220	0.604878	0.699729	0.806504	0.351220	0.201626	0.139946	0.080650	0.351220	0.604878	0.699729	0.806504	0.572931	0.498729	0.506695
2772	0.841800	No log	0.369106	0.610840	0.708943	0.808672	0.369106	0.203613	0.141789	0.080867	0.369106	0.610840	0.708943	0.808672	0.581510	0.509378	0.517100
2800	0.841800	No log	0.351762	0.610298	0.706233	0.813550	0.351762	0.203433	0.141247	0.081355	0.351762	0.610298	0.706233	0.813550	0.575980	0.500630	0.507818
2900	0.841800	No log	0.366938	0.603252	0.698103	0.803794	0.366938	0.201084	0.139621	0.080379	0.366938	0.603252	0.698103	0.803794	0.578786	0.507384	0.515327
3000	0.727900	No log	0.376694	0.616802	0.696477	0.810298	0.376694	0.205601	0.139295	0.081030	0.376694	0.616802	0.696477	0.810298	0.586976	0.516222	0.523838
3100	0.727900	No log	0.377778	0.612466	0.701355	0.808130	0.377778	0.204155	0.140271	0.080813	0.377778	0.612466	0.701355	0.808130	0.585881	0.515508	0.523467
3200	0.727900	No log	0.373984	0.623306	0.720325	0.811924	0.373984	0.207769	0.144065	0.081192	0.373984	0.623306	0.720325	0.811924	0.588734	0.517628	0.525620
3234	0.727900	No log	0.387534	0.620596	0.710027	0.814092	0.387534	0.206865	0.142005	0.081409	0.387534	0.620596	0.710027	0.814092	0.595261	0.525857	0.533548
3300	0.727900	No log	0.384282	0.622222	0.718699	0.820054	0.384282	0.207407	0.143740	0.082005	0.384282	0.622222	0.718699	0.820054	0.596560	0.525519	0.533022
3400	0.727900	No log	0.383198	0.629810	0.715447	0.818428	0.383198	0.209937	0.143089	0.081843	0.383198	0.629810	0.715447	0.818428	0.597156	0.526710	0.534413
3500	0.613500	No log	0.385908	0.634146	0.715989	0.814634	0.385908	0.211382	0.143198	0.081463	0.385908	0.634146	0.715989	0.814634	0.597851	0.528718	0.536614
3600	0.613500	No log	0.394580	0.633604	0.717615	0.817344	0.394580	0.211201	0.143523	0.081734	0.394580	0.633604	0.717615	0.817344	0.602171	0.533713	0.541328
3696	0.613500	No log	0.400542	0.637940	0.720325	0.823848	0.400542	0.212647	0.144065	0.082385	0.400542	0.637940	0.720325	0.823848	0.608205	0.539641	0.547058
3700	0.613500	No log	0.400000	0.634688	0.720325	0.823848	0.400000	0.211563	0.144065	0.082385	0.400000	0.634688	0.720325	0.823848	0.606935	0.538052	0.545441
3800	0.613500	No log	0.397290	0.634688	0.721409	0.817344	0.397290	0.211563	0.144282	0.081734	0.397290	0.634688	0.721409	0.817344	0.602985	0.534712	0.542111
3900	0.613500	No log	0.397832	0.638482	0.723035	0.819512	0.397832	0.212827	0.144607	0.081951	0.397832	0.638482	0.723035	0.819512	0.604850	0.536445	0.544064
4000	0.497300	No log	0.389702	0.637940	0.730623	0.832520	0.389702	0.212647	0.146125	0.083252	0.389702	0.637940	0.730623	0.832520	0.605803	0.533664	0.540397
4100	0.497300	No log	0.394038	0.641734	0.734959	0.839566	0.394038	0.213911	0.146992	0.083957	0.394038	0.641734	0.734959	0.839566	0.610809	0.538130	0.544152
4158	0.497300	No log	0.404336	0.646612	0.736043	0.831436	0.404336	0.215537	0.147209	0.083144	0.404336	0.646612	0.736043	0.831436	0.612517	0.542829	0.549797
4200	0.497300	No log	0.390244	0.643360	0.729539	0.826558	0.390244	0.214453	0.145908	0.082656	0.390244	0.643360	0.729539	0.826558	0.605507	0.534902	0.542089
4300	0.497300	No log	0.405420	0.653117	0.731707	0.833062	0.405420	0.217706	0.146341	0.083306	0.405420	0.653117	0.731707	0.833062	0.616689	0.547766	0.554415
4400	0.497300	No log	0.397832	0.645528	0.745257	0.831978	0.397832	0.215176	0.149051	0.083198	0.397832	0.645528	0.745257	0.831978	0.611714	0.541428	0.548367
4500	0.457800	No log	0.395122	0.641192	0.740379	0.834688	0.395122	0.213731	0.148076	0.083469	0.395122	0.641192	0.740379	0.834688	0.610879	0.539547	0.546138
4600	0.457800	No log	0.397832	0.651491	0.735501	0.823848	0.397832	0.217164	0.147100	0.082385	0.397832	0.651491	0.735501	0.823848	0.610758	0.542375	0.549932
4620	0.457800	No log	0.392412	0.654201	0.737669	0.825474	0.392412	0.218067	0.147534	0.082547	0.392412	0.654201	0.737669	0.825474	0.610686	0.541654	0.549322
4700	0.457800	No log	0.403794	0.667751	0.740379	0.837398	0.403794	0.222584	0.148076	0.083740	0.403794	0.667751	0.740379	0.837398	0.619971	0.550464	0.557227
4800	0.457800	No log	0.406504	0.658537	0.747425	0.833062	0.406504	0.219512	0.149485	0.083306	0.406504	0.658537	0.747425	0.833062	0.619502	0.551085	0.557884
4900	0.457800	No log	0.401626	0.659621	0.750678	0.836314	0.401626	0.219874	0.150136	0.083631	0.401626	0.659621	0.750678	0.836314	0.618198	0.548252	0.555062
5000	0.386100	No log	0.411382	0.656369	0.743089	0.834688	0.411382	0.218790	0.148618	0.083469	0.411382	0.656369	0.743089	0.834688	0.620154	0.551776	0.559027
5082	0.386100	No log	0.413550	0.656369	0.747967	0.831978	0.413550	0.218790	0.149593	0.083198	0.413550	0.656369	0.747967	0.831978	0.620618	0.552999	0.560084
5100	0.386100	No log	0.410298	0.651491	0.750678	0.829268	0.410298	0.217164	0.150136	0.082927	0.410298	0.651491	0.750678	0.829268	0.618521	0.550950	0.558255
5200	0.386100	No log	0.413550	0.657453	0.743089	0.831436	0.413550	0.219151	0.148618	0.083144	0.413550	0.657453	0.743089	0.831436	0.620944	0.553692	0.560722
5300	0.386100	No log	0.418428	0.663957	0.743089	0.829268	0.418428	0.221319	0.148618	0.082927	0.418428	0.663957	0.743089	0.829268	0.623435	0.557513	0.564873
5400	0.386100	No log	0.409756	0.662873	0.742005	0.833604	0.409756	0.220958	0.148401	0.083360	0.409756	0.662873	0.742005	0.833604	0.620964	0.552914	0.559865
5500	0.362700	No log	0.413550	0.665041	0.750136	0.839024	0.413550	0.221680	0.150027	0.083902	0.413550	0.665041	0.750136	0.839024	0.624397	0.555707	0.562361
5544	0.362700	No log	0.420054	0.662873	0.744173	0.842818	0.420054	0.220958	0.148835	0.084282	0.420054	0.662873	0.744173	0.842818	0.627662	0.559157	0.565582
5600	0.362700	No log	0.419512	0.673171	0.751220	0.842276	0.419512	0.224390	0.150244	0.084228	0.419512	0.673171	0.751220	0.842276	0.629490	0.561408	0.567874
5700	0.362700	No log	0.411924	0.668293	0.751220	0.840650	0.411924	0.222764	0.150244	0.084065	0.411924	0.668293	0.751220	0.840650	0.624870	0.555815	0.562220
5800	0.362700	No log	0.411924	0.667209	0.753930	0.841192	0.411924	0.222403	0.150786	0.084119	0.411924	0.667209	0.753930	0.841192	0.624878	0.555663	0.562285
5900	0.362700	No log	0.416260	0.682927	0.760976	0.846612	0.416260	0.227642	0.152195	0.084661	0.416260	0.682927	0.760976	0.846612	0.632042	0.563109	0.569488
6000	0.335000	No log	0.421680	0.678591	0.759350	0.850407	0.421680	0.226197	0.151870	0.085041	0.421680	0.678591	0.759350	0.850407	0.634592	0.565568	0.571717
6006	0.335000	No log	0.424390	0.678591	0.760434	0.848238	0.424390	0.226197	0.152087	0.084824	0.424390	0.678591	0.760434	0.848238	0.634824	0.566543	0.572886
6100	0.335000	No log	0.424390	0.673171	0.753930	0.850407	0.424390	0.224390	0.150786	0.085041	0.424390	0.673171	0.753930	0.850407	0.635013	0.566284	0.572503
6200	0.335000	No log	0.420596	0.675339	0.759892	0.842276	0.420596	0.225113	0.151978	0.084228	0.420596	0.675339	0.759892	0.842276	0.630715	0.562869	0.569648
6300	0.335000	No log	0.415176	0.678591	0.760976	0.847154	0.415176	0.226197	0.152195	0.084715	0.415176	0.678591	0.760976	0.847154	0.631165	0.561880	0.568384
6400	0.335000	No log	0.424932	0.681301	0.766938	0.847696	0.424932	0.227100	0.153388	0.084770	0.424932	0.681301	0.766938	0.847696	0.636497	0.568594	0.575037
6468	0.335000	No log	0.427642	0.684553	0.765854	0.845528	0.427642	0.228184	0.153171	0.084553	0.427642	0.684553	0.765854	0.845528	0.637392	0.570488	0.576941
6500	0.309400	No log	0.424390	0.683469	0.762060	0.843360	0.424390	0.227823	0.152412	0.084336	0.424390	0.683469	0.762060	0.843360	0.635208	0.568255	0.574974
6600	0.309400	No log	0.429810	0.678049	0.766396	0.840108	0.429810	0.226016	0.153279	0.084011	0.429810	0.678049	0.766396	0.840108	0.636002	0.570261	0.577452
6700	0.309400	No log	0.427100	0.678591	0.758808	0.847696	0.427100	0.226197	0.151762	0.084770	0.427100	0.678591	0.758808	0.847696	0.637163	0.569765	0.576251
6800	0.309400	No log	0.428184	0.678049	0.751762	0.842276	0.428184	0.226016	0.150352	0.084228	0.428184	0.678049	0.751762	0.842276	0.633935	0.567298	0.574149
6900	0.309400	No log	0.428184	0.679133	0.752304	0.839566	0.428184	0.226378	0.150461	0.083957	0.428184	0.679133	0.752304	0.839566	0.633607	0.567622	0.574459
6930	0.309400	No log	0.426558	0.677507	0.758808	0.841734	0.426558	0.225836	0.151762	0.084173	0.426558	0.677507	0.758808	0.841734	0.634278	0.567722	0.574593
7000	0.252600	No log	0.431436	0.686179	0.762602	0.846070	0.431436	0.228726	0.152520	0.084607	0.431436	0.686179	0.762602	0.846070	0.638771	0.572272	0.578839
7100	0.252600	No log	0.429268	0.688889	0.760434	0.851491	0.429268	0.229630	0.152087	0.085149	0.429268	0.688889	0.760434	0.851491	0.640961	0.573485	0.579630
7200	0.252600	No log	0.424390	0.692683	0.765312	0.847696	0.424390	0.230894	0.153062	0.084770	0.424390	0.692683	0.765312	0.847696	0.638203	0.570799	0.577108
7300	0.252600	No log	0.422222	0.685095	0.766938	0.852033	0.422222	0.228365	0.153388	0.085203	0.422222	0.685095	0.766938	0.852033	0.638132	0.569398	0.575422
7392	0.252600	No log	0.415176	0.688889	0.774526	0.861247	0.415176	0.229630	0.154905	0.086125	0.415176	0.688889	0.774526	0.861247	0.639304	0.568045	0.573470
7400	0.252600	No log	0.416260	0.688347	0.770190	0.856911	0.416260	0.229449	0.154038	0.085691	0.416260	0.688347	0.770190	0.856911	0.637972	0.567566	0.573385
7500	0.228700	No log	0.419512	0.685095	0.766396	0.855827	0.419512	0.228365	0.153279	0.085583	0.419512	0.685095	0.766396	0.855827	0.637743	0.567753	0.573509
7600	0.228700	No log	0.419512	0.674255	0.765854	0.856369	0.419512	0.224752	0.153171	0.085637	0.419512	0.674255	0.765854	0.856369	0.636626	0.566388	0.572121
7700	0.228700	No log	0.416802	0.686721	0.765312	0.853117	0.416802	0.228907	0.153062	0.085312	0.416802	0.686721	0.765312	0.853117	0.636078	0.566368	0.572373
7800	0.228700	No log	0.421138	0.679133	0.760434	0.852033	0.421138	0.226378	0.152087	0.085203	0.421138	0.679133	0.760434	0.852033	0.636152	0.567094	0.573281
7854	0.228700	No log	0.430894	0.686179	0.766938	0.857453	0.430894	0.228726	0.153388	0.085745	0.430894	0.686179	0.766938	0.857453	0.643794	0.575419	0.581139
7900	0.228700	No log	0.425474	0.677507	0.758266	0.851491	0.425474	0.225836	0.151653	0.085149	0.425474	0.677507	0.758266	0.851491	0.638048	0.569658	0.575841
8000	0.229600	No log	0.429810	0.681843	0.764228	0.848780	0.429810	0.227281	0.152846	0.084878	0.429810	0.681843	0.764228	0.848780	0.639529	0.572435	0.578672
8100	0.229600	No log	0.421138	0.681843	0.763686	0.844986	0.421138	0.227281	0.152737	0.084499	0.421138	0.681843	0.763686	0.844986	0.634371	0.566651	0.573227
8200	0.229600	No log	0.422222	0.693225	0.765854	0.842818	0.422222	0.231075	0.153171	0.084282	0.422222	0.693225	0.765854	0.842818	0.635731	0.568915	0.575773
8300	0.229600	No log	0.417344	0.686179	0.773442	0.849864	0.417344	0.228726	0.154688	0.084986	0.417344	0.686179	0.773442	0.849864	0.636273	0.567425	0.573464
8316	0.229600	No log	0.422764	0.687805	0.772358	0.846612	0.422764	0.229268	0.154472	0.084661	0.422764	0.687805	0.772358	0.846612	0.637898	0.570518	0.577061
8400	0.229600	No log	0.427100	0.693225	0.774526	0.846070	0.427100	0.231075	0.154905	0.084607	0.427100	0.693225	0.774526	0.846070	0.640414	0.573890	0.580357
8500	0.240900	No log	0.429268	0.696477	0.771816	0.852033	0.429268	0.232159	0.154363	0.085203	0.429268	0.696477	0.771816	0.852033	0.643246	0.575915	0.582059
8600	0.240900	No log	0.432520	0.698103	0.769106	0.848238	0.432520	0.232701	0.153821	0.084824	0.432520	0.698103	0.769106	0.848238	0.643334	0.577230	0.583745
8700	0.240900	No log	0.437398	0.695935	0.770190	0.847154	0.437398	0.231978	0.154038	0.084715	0.437398	0.695935	0.770190	0.847154	0.644845	0.579587	0.585846
8778	0.240900	No log	0.435230	0.694309	0.767480	0.846070	0.435230	0.231436	0.153496	0.084607	0.435230	0.694309	0.767480	0.846070	0.643135	0.577707	0.584279
8800	0.240900	No log	0.440108	0.692141	0.765854	0.848238	0.440108	0.230714	0.153171	0.084824	0.440108	0.692141	0.765854	0.848238	0.645336	0.580128	0.586430
8900	0.240900	No log	0.438482	0.694851	0.777236	0.846612	0.438482	0.231617	0.155447	0.084661	0.438482	0.694851	0.777236	0.846612	0.645193	0.580219	0.586576
9000	0.202800	No log	0.440108	0.688889	0.775610	0.849322	0.440108	0.229630	0.155122	0.084932	0.440108	0.688889	0.775610	0.849322	0.646050	0.580650	0.586830
9100	0.202800	No log	0.440108	0.693225	0.781030	0.849322	0.440108	0.231075	0.156206	0.084932	0.440108	0.693225	0.781030	0.849322	0.646934	0.581626	0.587885
9200	0.202800	No log	0.443902	0.702981	0.774526	0.849864	0.443902	0.234327	0.154905	0.084986	0.443902	0.702981	0.774526	0.849864	0.650946	0.586649	0.592925
9240	0.202800	No log	0.443360	0.705149	0.776152	0.853117	0.443360	0.235050	0.155230	0.085312	0.443360	0.705149	0.776152	0.853117	0.651321	0.586258	0.592317
9300	0.202800	No log	0.443360	0.705691	0.775610	0.855285	0.443360	0.235230	0.155122	0.085528	0.443360	0.705691	0.775610	0.855285	0.651867	0.586357	0.592210
9400	0.202800	No log	0.437398	0.704065	0.770732	0.849864	0.437398	0.234688	0.154146	0.084986	0.437398	0.704065	0.770732	0.849864	0.647151	0.581799	0.587967
9500	0.205700	No log	0.443360	0.706233	0.773984	0.852033	0.443360	0.235411	0.154797	0.085203	0.443360	0.706233	0.773984	0.852033	0.650329	0.585407	0.591648
9600	0.205700	No log	0.441734	0.701355	0.782114	0.853659	0.441734	0.233785	0.156423	0.085366	0.441734	0.701355	0.782114	0.853659	0.651138	0.585725	0.591723
9700	0.205700	No log	0.450949	0.702439	0.776152	0.854743	0.450949	0.234146	0.155230	0.085474	0.450949	0.702439	0.776152	0.854743	0.654893	0.590601	0.596675
9702	0.205700	No log	0.451491	0.703523	0.776152	0.853659	0.451491	0.234508	0.155230	0.085366	0.451491	0.703523	0.776152	0.853659	0.655189	0.591250	0.597423
9800	0.205700	No log	0.441734	0.705691	0.776694	0.849864	0.441734	0.235230	0.155339	0.084986	0.441734	0.705691	0.776694	0.849864	0.649872	0.585226	0.591509
9900	0.205700	No log	0.435230	0.700271	0.776152	0.845528	0.435230	0.233424	0.155230	0.084553	0.435230	0.700271	0.776152	0.845528	0.645567	0.580860	0.587368
10000	0.221100	No log	0.437940	0.699187	0.775610	0.847154	0.437940	0.233062	0.155122	0.084715	0.437940	0.699187	0.775610	0.847154	0.646506	0.581703	0.588190
10100	0.221100	No log	0.446070	0.705691	0.775068	0.849322	0.446070	0.235230	0.155014	0.084932	0.446070	0.705691	0.775068	0.849322	0.650922	0.586936	0.593315
10164	0.221100	No log	0.446612	0.705149	0.772358	0.854201	0.446612	0.235050	0.154472	0.085420	0.446612	0.705149	0.772358	0.854201	0.652829	0.587979	0.594206
10200	0.221100	No log	0.448238	0.703523	0.776694	0.857453	0.448238	0.234508	0.155339	0.085745	0.448238	0.703523	0.776694	0.857453	0.653946	0.588534	0.594433
10300	0.221100	No log	0.442276	0.700813	0.776152	0.855285	0.442276	0.233604	0.155230	0.085528	0.442276	0.700813	0.776152	0.855285	0.650535	0.584639	0.590834
10400	0.221100	No log	0.443360	0.705691	0.777236	0.856369	0.443360	0.235230	0.155447	0.085637	0.443360	0.705691	0.777236	0.856369	0.652588	0.586968	0.593074
10500	0.174900	No log	0.443360	0.704607	0.777236	0.857453	0.443360	0.234869	0.155447	0.085745	0.443360	0.704607	0.777236	0.857453	0.653029	0.587195	0.593227
10600	0.174900	No log	0.446070	0.705149	0.776694	0.855827	0.446070	0.235050	0.155339	0.085583	0.446070	0.705149	0.776694	0.855827	0.654463	0.589544	0.595752
10626	0.174900	No log	0.452033	0.699729	0.776694	0.853659	0.452033	0.233243	0.155339	0.085366	0.452033	0.699729	0.776694	0.853659	0.655401	0.591541	0.597962
10700	0.174900	No log	0.445528	0.702981	0.772358	0.856911	0.445528	0.234327	0.154472	0.085691	0.445528	0.702981	0.772358	0.856911	0.653187	0.587669	0.593802
10800	0.174900	No log	0.445528	0.701355	0.770190	0.856369	0.445528	0.233785	0.154038	0.085637	0.445528	0.701355	0.770190	0.856369	0.652416	0.586862	0.592906
10900	0.174900	No log	0.446612	0.702981	0.773984	0.856369	0.446612	0.234327	0.154797	0.085637	0.446612	0.702981	0.773984	0.856369	0.653087	0.587796	0.594073
11000	0.173100	No log	0.450949	0.701897	0.776152	0.856369	0.450949	0.233966	0.155230	0.085637	0.450949	0.701897	0.776152	0.856369	0.655047	0.590382	0.596682
11088	0.173100	No log	0.454743	0.701897	0.777236	0.855285	0.454743	0.233966	0.155447	0.085528	0.454743	0.701897	0.777236	0.855285	0.656252	0.592307	0.598718
11100	0.173100	No log	0.455285	0.702439	0.776694	0.857453	0.455285	0.234146	0.155339	0.085745	0.455285	0.702439	0.776694	0.857453	0.656917	0.592589	0.598785
11200	0.173100	No log	0.449864	0.706775	0.781030	0.857995	0.449864	0.235592	0.156206	0.085799	0.449864	0.706775	0.781030	0.857995	0.656316	0.591397	0.597346
11300	0.173100	No log	0.452575	0.704065	0.783198	0.857995	0.452575	0.234688	0.156640	0.085799	0.452575	0.704065	0.783198	0.857995	0.657569	0.593064	0.599114
11400	0.173100	No log	0.454201	0.701355	0.778862	0.856369	0.454201	0.233785	0.155772	0.085637	0.454201	0.701355	0.778862	0.856369	0.656903	0.592831	0.599058
11500	0.176300	No log	0.450949	0.700813	0.777236	0.858537	0.450949	0.233604	0.155447	0.085854	0.450949	0.700813	0.777236	0.858537	0.655784	0.590680	0.596756
11550	0.176300	No log	0.452575	0.703523	0.774526	0.857453	0.452575	0.234508	0.154905	0.085745	0.452575	0.703523	0.774526	0.857453	0.655989	0.591288	0.597476
11600	0.176300	No log	0.451491	0.701897	0.776152	0.859079	0.451491	0.233966	0.155230	0.085908	0.451491	0.701897	0.776152	0.859079	0.656642	0.591630	0.597632
11700	0.176300	No log	0.449864	0.704065	0.776694	0.862331	0.449864	0.234688	0.155339	0.086233	0.449864	0.704065	0.776694	0.862331	0.657290	0.591546	0.597194
11800	0.176300	No log	0.452033	0.709485	0.778320	0.860705	0.452033	0.236495	0.155664	0.086070	0.452033	0.709485	0.778320	0.860705	0.658616	0.593648	0.599529
11900	0.176300	No log	0.450407	0.704065	0.779404	0.861247	0.450407	0.234688	0.155881	0.086125	0.450407	0.704065	0.779404	0.861247	0.657712	0.592307	0.598087
12000	0.156900	No log	0.452575	0.704607	0.780488	0.861789	0.452575	0.234869	0.156098	0.086179	0.452575	0.704607	0.780488	0.861789	0.659461	0.594396	0.600203
12012	0.156900	No log	0.452575	0.705149	0.781572	0.860705	0.452575	0.235050	0.156314	0.086070	0.452575	0.705149	0.781572	0.860705	0.659051	0.594172	0.600034
12100	0.156900	No log	0.451491	0.705691	0.782114	0.860163	0.451491	0.235230	0.156423	0.086016	0.451491	0.705691	0.782114	0.860163	0.658517	0.593633	0.599407
12200	0.156900	No log	0.455827	0.705149	0.783740	0.859079	0.455827	0.235050	0.156748	0.085908	0.455827	0.705149	0.783740	0.859079	0.660673	0.596759	0.602729
12300	0.156900	No log	0.449864	0.705149	0.781030	0.860705	0.449864	0.235050	0.156206	0.086070	0.449864	0.705149	0.781030	0.860705	0.658411	0.593265	0.599210
12400	0.156900	No log	0.449322	0.703523	0.783198	0.860163	0.449322	0.234508	0.156640	0.086016	0.449322	0.703523	0.783198	0.860163	0.658309	0.593275	0.599201
12474	0.156900	No log	0.450949	0.706233	0.784282	0.859079	0.450949	0.235411	0.156856	0.085908	0.450949	0.706233	0.784282	0.859079	0.659206	0.594706	0.600754
12500	0.150400	No log	0.447696	0.706775	0.784824	0.860705	0.447696	0.235592	0.156965	0.086070	0.447696	0.706775	0.784824	0.860705	0.658063	0.592677	0.598619
12600	0.150400	No log	0.447696	0.706775	0.785908	0.859621	0.447696	0.235592	0.157182	0.085962	0.447696	0.706775	0.785908	0.859621	0.657739	0.592581	0.598529
12700	0.150400	No log	0.450949	0.706775	0.783198	0.859621	0.450949	0.235592	0.156640	0.085962	0.450949	0.706775	0.783198	0.859621	0.658981	0.594270	0.600180
12800	0.150400	No log	0.448238	0.702439	0.781572	0.857995	0.448238	0.234146	0.156314	0.085799	0.448238	0.702439	0.781572	0.857995	0.656685	0.591735	0.597804
12900	0.150400	No log	0.450407	0.704607	0.781572	0.860163	0.450407	0.234869	0.156314	0.086016	0.450407	0.704607	0.781572	0.860163	0.658746	0.593849	0.599709
12936	0.150400	No log	0.449864	0.702981	0.781572	0.861247	0.449864	0.234327	0.156314	0.086125	0.449864	0.702981	0.781572	0.861247	0.658468	0.593199	0.599003
13000	0.151600	No log	0.447696	0.700271	0.782656	0.859621	0.447696	0.233424	0.156531	0.085962	0.447696	0.700271	0.782656	0.859621	0.656966	0.591691	0.597634
13100	0.151600	No log	0.448780	0.704607	0.779946	0.861247	0.448780	0.234869	0.155989	0.086125	0.448780	0.704607	0.779946	0.861247	0.658043	0.592667	0.598390
13200	0.151600	No log	0.451491	0.705149	0.781030	0.860163	0.451491	0.235050	0.156206	0.086016	0.451491	0.705149	0.781030	0.860163	0.658654	0.593789	0.599678
13300	0.151600	No log	0.449864	0.706233	0.780488	0.860705	0.449864	0.235411	0.156098	0.086070	0.449864	0.706233	0.780488	0.860705	0.658486	0.593370	0.599182
13398	0.151600	No log	0.449864	0.706775	0.781030	0.859079	0.449864	0.235592	0.156206	0.085908	0.449864	0.706775	0.781030	0.859079	0.658333	0.593608	0.599570
13400	0.151600	No log	0.450407	0.706775	0.781030	0.859079	0.450407	0.235592	0.156206	0.085908	0.450407	0.706775	0.781030	0.859079	0.658557	0.593907	0.599874
13500	0.133800	No log	0.448780	0.707859	0.780488	0.858537	0.448780	0.235953	0.156098	0.085854	0.448780	0.707859	0.780488	0.858537	0.657703	0.592917	0.598979
13600	0.133800	No log	0.451491	0.707859	0.780488	0.858537	0.451491	0.235953	0.156098	0.085854	0.451491	0.707859	0.780488	0.858537	0.658833	0.594459	0.600548
13700	0.133800	No log	0.452033	0.707317	0.779404	0.857995	0.452033	0.235772	0.155881	0.085799	0.452033	0.707317	0.779404	0.857995	0.658758	0.594526	0.600652
13800	0.133800	No log	0.452575	0.707317	0.779946	0.857453	0.452575	0.235772	0.155989	0.085745	0.452575	0.707317	0.779946	0.857453	0.658976	0.594962	0.601124
13860	0.133800	No log	0.452575	0.707317	0.779946	0.857453	0.452575	0.235772	0.155989	0.085745	0.452575	0.707317	0.779946	0.857453	0.658986	0.594972	0.601133




+++++++++++++++++++

from sentence_transformers import SentenceTransformerTrainingArguments
from sentence_transformers.training_args import BatchSamplers
outpath = 'finetuned_tdb_paraphrase-multilingual_mpnet_try4'
args = SentenceTransformerTrainingArguments(
    output_dir=outpath,
    # Optional training parameters:
    num_train_epochs=10,
    per_device_train_batch_size=64  ,# Increase if GPU memory allows
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    learning_rate=1e-5,
    warmup_ratio=0.1,
    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16
    bf16=False,  # Set to True if you have a GPU that supports BF16
    batch_sampler=BatchSamplers.NO_DUPLICATES,  # losses that use "in-batch negatives" benefit from no duplicates
    # Optional tracking/debugging parameters:
    eval_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    save_total_limit=2,
    logging_steps=100,
    run_name="mpnet-base-bioasq-basic-training-args",
    weight_decay=0.01,
    lr_scheduler_type="cosine"   # Will be used in W&B if `wandb` is installed
)
from sentence_transformers import SentenceTransformerTrainer

trainer = SentenceTransformerTrainer(
    model=model,
    args=args,
    train_dataset=train_dataset_1.select_columns(["anchor", "positive"]),
    loss=train_loss,
    evaluator=evaluator,
)


[580/580 30:34, Epoch 10/10]
Step	Training Loss	Validation Loss	Cosine Accuracy@1	Cosine Accuracy@3	Cosine Accuracy@5	Cosine Accuracy@10	Cosine Precision@1	Cosine Precision@3	Cosine Precision@5	Cosine Precision@10	Cosine Recall@1	Cosine Recall@3	Cosine Recall@5	Cosine Recall@10	Cosine Ndcg@10	Cosine Mrr@10	Cosine Map@100
50	No log	No log	0.443902	0.691599	0.767480	0.856911	0.443902	0.230533	0.153496	0.085691	0.443902	0.691599	0.767480	0.856911	0.649096	0.582706	0.588587
100	1.069900	No log	0.444986	0.694309	0.771274	0.857995	0.444986	0.231436	0.154255	0.085799	0.444986	0.694309	0.771274	0.857995	0.651598	0.585500	0.591280
150	1.069900	No log	0.448238	0.690515	0.771274	0.857453	0.448238	0.230172	0.154255	0.085745	0.448238	0.690515	0.771274	0.857453	0.651800	0.586035	0.591807
200	1.156800	No log	0.448238	0.699187	0.769648	0.851491	0.448238	0.233062	0.153930	0.085149	0.448238	0.699187	0.769648	0.851491	0.651799	0.587623	0.594167
250	1.156800	No log	0.445528	0.700813	0.769648	0.861247	0.445528	0.233604	0.153930	0.086125	0.445528	0.700813	0.769648	0.861247	0.654263	0.587921	0.593503
300	1.139200	No log	0.450407	0.705691	0.776694	0.859079	0.450407	0.235230	0.155339	0.085908	0.450407	0.705691	0.776694	0.859079	0.656855	0.591868	0.597664
350	1.139200	No log	0.455285	0.703523	0.773984	0.860705	0.455285	0.234508	0.154797	0.086070	0.455285	0.703523	0.773984	0.860705	0.658995	0.594334	0.599995
400	0.965600	No log	0.456911	0.698103	0.771816	0.857995	0.456911	0.232701	0.154363	0.085799	0.456911	0.698103	0.771816	0.857995	0.658172	0.594110	0.599885
450	0.965600	No log	0.455827	0.699187	0.774526	0.858537	0.455827	0.233062	0.154905	0.085854	0.455827	0.699187	0.774526	0.858537	0.657552	0.593171	0.598954
500	0.969800	No log	0.455827	0.699729	0.775610	0.860163	0.455827	0.233243	0.155122	0.086016	0.455827	0.699729	0.775610	0.860163	0.658163	0.593480	0.599099
550	0.969800	No log	0.459079	0.700271	0.775610	0.861789	0.459079	0.233424	0.155122	0.086179	0.459079	0.700271	0.775610	0.861789	0.659774	0.595197	0.600687
TrainOutput(global_step=580, training_loss=1.0364808312777816, metrics={'train_runtime': 1835.7128, 'train_samples_per_second': 40.197, 'train_steps_per_second': 0.316, 'total_flos': 0.0, 'train_loss': 1.0364808312777816, 'epoch': 10.0})



++++++++++++++++++++++

from sentence_transformers import SentenceTransformerTrainingArguments
from sentence_transformers.training_args import BatchSamplers
outpath = 'finetuned_tdb_paraphrase-multilingual_mpnet_try4'
args = SentenceTransformerTrainingArguments(
    output_dir=outpath,
    # Optional training parameters:
    num_train_epochs=30,
    per_device_train_batch_size=64  ,# Increase if GPU memory allows
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    learning_rate=1e-5,
    warmup_ratio=0.1,
    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16
    bf16=False,  # Set to True if you have a GPU that supports BF16
    batch_sampler=BatchSamplers.NO_DUPLICATES,  # losses that use "in-batch negatives" benefit from no duplicates
    # Optional tracking/debugging parameters:
    eval_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=50,
    save_total_limit=2,
    logging_steps=100,
    run_name="mpnet-base-bioasq-basic-training-args",
    weight_decay=0.01,
    lr_scheduler_type="cosine"   # Will be used in W&B if `wandb` is installed
)
from sentence_transformers import SentenceTransformerTrainer

trainer = SentenceTransformerTrainer(
    model=model,
    args=args,
    train_dataset=train_dataset_1.select_columns(["anchor", "positive"]),
    loss=train_loss,
    evaluator=evaluator,
)


wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)
wandb: You can find your API key in your browser here: https://wandb.ai/authorize
wandb: Paste an API key from your profile and hit enter:

 ··········

wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.
wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.
wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc
wandb: Currently logged in as: gmunkhtur to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.

Tracking run with wandb version 0.19.6
Run data is saved locally in /content/wandb/run-20250219_060704-rixxqyjn
Syncing run mpnet-base-bioasq-basic-training-args to Weights & Biases (docs)
View project at https://wandb.ai/gmunkhtur/sentence-transformers
View run at https://wandb.ai/gmunkhtur/sentence-transformers/runs/rixxqyjn
[1740/1740 56:43, Epoch 30/30]
Step 	Training Loss 	Validation Loss 	Cosine Accuracy@1 	Cosine Accuracy@3 	Cosine Accuracy@5 	Cosine Accuracy@10 	Cosine Precision@1 	Cosine Precision@3 	Cosine Precision@5 	Cosine Precision@10 	Cosine Recall@1 	Cosine Recall@3 	Cosine Recall@5 	Cosine Recall@10 	Cosine Ndcg@10 	Cosine Mrr@10 	Cosine Map@100
50 	No log 	No log 	0.195664 	0.325203 	0.388618 	0.487805 	0.195664 	0.108401 	0.077724 	0.048780 	0.195664 	0.325203 	0.388618 	0.487805 	0.328538 	0.279127 	0.291829
100 	23.422500 	No log 	0.262331 	0.442276 	0.512737 	0.605962 	0.262331 	0.147425 	0.102547 	0.060596 	0.262331 	0.442276 	0.512737 	0.605962 	0.426917 	0.370520 	0.382403
150 	23.422500 	No log 	0.296477 	0.492683 	0.568564 	0.665041 	0.296477 	0.164228 	0.113713 	0.066504 	0.296477 	0.492683 	0.568564 	0.665041 	0.473927 	0.413618 	0.425114
200 	12.754700 	No log 	0.321951 	0.531707 	0.613550 	0.706775 	0.321951 	0.177236 	0.122710 	0.070678 	0.321951 	0.531707 	0.613550 	0.706775 	0.509830 	0.447381 	0.458623
250 	12.754700 	No log 	0.345799 	0.560976 	0.639566 	0.743631 	0.345799 	0.186992 	0.127913 	0.074363 	0.345799 	0.560976 	0.639566 	0.743631 	0.538683 	0.473904 	0.484051
300 	8.070800 	No log 	0.356098 	0.580488 	0.657995 	0.757724 	0.356098 	0.193496 	0.131599 	0.075772 	0.356098 	0.580488 	0.657995 	0.757724 	0.551275 	0.485828 	0.496132
350 	8.070800 	No log 	0.366396 	0.601084 	0.675881 	0.782656 	0.366396 	0.200361 	0.135176 	0.078266 	0.366396 	0.601084 	0.675881 	0.782656 	0.568851 	0.501146 	0.510048
400 	5.775600 	No log 	0.373442 	0.616802 	0.689431 	0.791870 	0.373442 	0.205601 	0.137886 	0.079187 	0.373442 	0.616802 	0.689431 	0.791870 	0.577914 	0.509914 	0.518785
450 	5.775600 	No log 	0.381030 	0.631978 	0.708943 	0.803794 	0.381030 	0.210659 	0.141789 	0.080379 	0.381030 	0.631978 	0.708943 	0.803794 	0.590606 	0.522552 	0.531052
500 	4.412200 	No log 	0.385908 	0.636314 	0.722493 	0.814092 	0.385908 	0.212105 	0.144499 	0.081409 	0.385908 	0.636314 	0.722493 	0.814092 	0.596497 	0.527093 	0.535079
550 	4.412200 	No log 	0.392412 	0.649322 	0.733875 	0.821138 	0.392412 	0.216441 	0.146775 	0.082114 	0.392412 	0.649322 	0.733875 	0.821138 	0.604938 	0.535758 	0.543397
600 	3.389400 	No log 	0.407588 	0.650407 	0.730623 	0.822764 	0.407588 	0.216802 	0.146125 	0.082276 	0.407588 	0.650407 	0.730623 	0.822764 	0.611698 	0.544377 	0.552092
650 	3.389400 	No log 	0.416802 	0.657453 	0.733333 	0.826016 	0.416802 	0.219151 	0.146667 	0.082602 	0.416802 	0.657453 	0.733333 	0.826016 	0.619677 	0.553741 	0.561289
700 	2.983600 	No log 	0.413008 	0.661247 	0.744715 	0.831436 	0.413008 	0.220416 	0.148943 	0.083144 	0.413008 	0.661247 	0.744715 	0.831436 	0.620637 	0.553161 	0.560573
750 	2.983600 	No log 	0.419512 	0.672087 	0.752304 	0.834146 	0.419512 	0.224029 	0.150461 	0.083415 	0.419512 	0.672087 	0.752304 	0.834146 	0.625945 	0.559129 	0.566376
800 	2.391000 	No log 	0.420596 	0.673713 	0.750678 	0.835772 	0.420596 	0.224571 	0.150136 	0.083577 	0.420596 	0.673713 	0.750678 	0.835772 	0.628544 	0.562030 	0.569162
850 	2.391000 	No log 	0.428726 	0.681301 	0.755556 	0.843360 	0.428726 	0.227100 	0.151111 	0.084336 	0.428726 	0.681301 	0.755556 	0.843360 	0.635190 	0.568522 	0.575227
900 	2.178400 	No log 	0.429268 	0.687805 	0.755014 	0.843902 	0.429268 	0.229268 	0.151003 	0.084390 	0.429268 	0.687805 	0.755014 	0.843902 	0.637825 	0.571673 	0.578435
950 	2.178400 	No log 	0.427642 	0.680759 	0.760434 	0.846070 	0.427642 	0.226920 	0.152087 	0.084607 	0.427642 	0.680759 	0.760434 	0.846070 	0.637095 	0.570100 	0.576598
1000 	1.917000 	No log 	0.426558 	0.689431 	0.761518 	0.845528 	0.426558 	0.229810 	0.152304 	0.084553 	0.426558 	0.689431 	0.761518 	0.845528 	0.638029 	0.571308 	0.577796
1050 	1.917000 	No log 	0.428184 	0.688889 	0.764228 	0.846070 	0.428184 	0.229630 	0.152846 	0.084607 	0.428184 	0.688889 	0.764228 	0.846070 	0.638420 	0.571629 	0.578053
1100 	1.735100 	No log 	0.434688 	0.684011 	0.764228 	0.847154 	0.434688 	0.228004 	0.152846 	0.084715 	0.434688 	0.684011 	0.764228 	0.847154 	0.641237 	0.575179 	0.581531
1150 	1.735100 	No log 	0.436314 	0.692683 	0.763686 	0.842276 	0.436314 	0.230894 	0.152737 	0.084228 	0.436314 	0.692683 	0.763686 	0.842276 	0.641013 	0.576162 	0.582981
1200 	1.648500 	No log 	0.438482 	0.696477 	0.764770 	0.851491 	0.438482 	0.232159 	0.152954 	0.085149 	0.438482 	0.696477 	0.764770 	0.851491 	0.646131 	0.580143 	0.586218
1250 	1.648500 	No log 	0.440650 	0.695935 	0.766938 	0.848780 	0.440650 	0.231978 	0.153388 	0.084878 	0.440650 	0.695935 	0.766938 	0.848780 	0.646659 	0.581669 	0.587918
1300 	1.567600 	No log 	0.443360 	0.692141 	0.766938 	0.852033 	0.443360 	0.230714 	0.153388 	0.085203 	0.443360 	0.692141 	0.766938 	0.852033 	0.647713 	0.582235 	0.588310
1350 	1.567600 	No log 	0.438482 	0.693225 	0.765312 	0.851491 	0.438482 	0.231075 	0.153062 	0.085149 	0.438482 	0.693225 	0.765312 	0.851491 	0.645602 	0.579564 	0.585689
1400 	1.480200 	No log 	0.437940 	0.690515 	0.763144 	0.852033 	0.437940 	0.230172 	0.152629 	0.085203 	0.437940 	0.690515 	0.763144 	0.852033 	0.645597 	0.579376 	0.585525
1450 	1.480200 	No log 	0.436856 	0.694309 	0.764228 	0.848780 	0.436856 	0.231436 	0.152846 	0.084878 	0.436856 	0.694309 	0.764228 	0.848780 	0.644376 	0.578648 	0.585088
1500 	1.447500 	No log 	0.434688 	0.695935 	0.766396 	0.850407 	0.434688 	0.231978 	0.153279 	0.085041 	0.434688 	0.695935 	0.766396 	0.850407 	0.644360 	0.578109 	0.584461
1550 	1.447500 	No log 	0.434146 	0.694851 	0.766396 	0.847696 	0.434146 	0.231617 	0.153279 	0.084770 	0.434146 	0.694851 	0.766396 	0.847696 	0.643015 	0.577100 	0.583683
1600 	1.433200 	No log 	0.434688 	0.696477 	0.764770 	0.849864 	0.434688 	0.232159 	0.152954 	0.084986 	0.434688 	0.696477 	0.764770 	0.849864 	0.644198 	0.578026 	0.584407
1650 	1.433200 	No log 	0.434688 	0.697561 	0.765312 	0.849322 	0.434688 	0.232520 	0.153062 	0.084932 	0.434688 	0.697561 	0.765312 	0.849322 	0.644044 	0.577956 	0.584394
1700 	1.445300 	No log 	0.434146 	0.696477 	0.765312 	0.849322 	0.434146 	0.232159 	0.153062 	0.084932 	0.434146 	0.696477 	0.765312 	0.849322 	0.643753 	0.577578 	0.584019

TrainOutput(global_step=1740, training_loss=4.517082448937427, metrics={'train_runtime': 3436.2318, 'train_samples_per_second': 64.422, 'train_steps_per_second': 0.506, 'total_flos': 0.0, 'train_loss': 4.517082448937427, 'epoch': 30.0})

page_content='Цалингийн зээл
Та онлайн банк ашиглан цалингийн зээлээ цахимаар аваарай.


Хэрэглээний зээл нь төлөвлөгөөт болон гэнэтийн санхүүгийн хэрэгцээг шийдвэрлэх боломжийг олгодог. Харилцагч та ХХБ-ны зээлийн олон төрлийн үйлчилгээг ашиглан санхүүгийн хэрэгцээгээ хангах боломжтой.

Зээлийн эрх үйлчилгээг ашиглан дансандаа байгаа үлдэгдэл дүнгээс илүү дүнг ашиглах, ногоон зээлээр эко хэрэглээгээрээ байгаль дэлхийгээ хайрлах, цахим зээлээр цагаа хэмнэх, тэтгэврийн зээлийн эрхээр тэтгэврийн орлогоо барьцаалан зээл авах, тэтгэврийн зээлээр ХХБ-аар дамжуулан тэтгэвэр авдаг ахмадуудад зориулсан уян хатан нөхцөлтэй зээл авах, хялбар зээлээр үл хөдлөх барьцаалан зээл авах боломжтой. Мөн хадгаламж барьцаалсан зээлийг ХХБ-нд хугацаатай хадгаламжтай харилцагч өөрийн хадгаламжаа барьцаалан банкны бүх салбар болон онлайнаар авах, цалингийн зээлийг онлайн банк ашиглан цахимаар авах боломжтой.
Зээлийн бүтээгдэхүүнүүдийн тайлбар ба хэрэглээний зээлийн ерөнхий мэдээлэл.
' metadata={'source': 'https://tdbm.mn/mn/retail/loans/heregleenii-zeel', 'relevance_score': 0.23669262}
++++++++++++++++++++++++++
page_content='Хэрэглээний зээл нь хувь хүний санхүүгийн хэрэгцээнд зориулсан олон төрлийн бүтээгдэхүүнээс бүрдэх бөгөөд үүнд цалингийн зээл, цахим хэрэглээний зээл, хялбар зээл, зээлийн эрх, ногоон зээл, тэтгэврийн зээл, тэтгэвэр болон хадгаламж барьцаалсан зээлийн эрх багтана.

Орон сууцны зээл нь орон сууц худалдан авах, засвар тохижилт хийх, хувийн орон сууц худалдан авах болон ногоон орон сууцны санхүүжилтийн шийдлүүдийг агуулна. Мөн авто зогсоол худалдан авах зээл болон 6%-ийн хүүтэй орон сууцны зээл багтана.

Автомашины зээл нь шинэ, хуучин, цахилгаан болон эко автомашин худалдан авах боломжийг бүрдүүлдэг.

Бизнесийн зээл нь аж ахуйн нэгжүүдийн үйл ажиллагааг дэмжих зорилготой бөгөөд бизнесийн зээл, ПОС-ын орлого барьцаалсан зээл, нийлүүлэлтийн сүлжээний зээлийн эрх, тоног төхөөрөмжийн зээл, бичил бизнес эрхлэгчдэд зориулсан зээл, урт хугацаат үл хөдлөх хөрөнгийн зээл болон эмэгтэй бизнес эрхлэгчдийг дэмжих зээлийг багтаадаг.
Зээлийн төрөл бүрийн тодорхойлолт, онцлог.
' metadata={'source': 'https://tdbm.mn/mn/retail/loans', 'relevance_score': 0.19559409}
++++++++++++++++++++++++++
page_content='# Хэрэглээний зээлТа төлөвлөгөөт болон гэнэтийн санхүүгийн хэрэгцээгээ хэрэглээний зээлээр шийдээрэй. 
Зээлийн эрх
Харилцагч та Зээлийн эрх үйлчилгээг ашиглан дансандаа байгаа үлдэгдэл дүнгээс илүү дүнг ашиглах боломжтой.

Ногоон зээл
Харилцагч та Ногоон зээлийн бүтээгдэхүүнийг сонгож, эко хэрэглээгээрээ байгаль дэлхийгээ хайрлаарай.

Хэрэглээний цахим зээл
Цахим зээлээр цагаа хэмнэн, хэрэглээгээ шийдэх боломжийг олгож байна.

Тэтгэврийн зээлийн эрх
Харилцагч танд яаралтай мөнгөний хэрэг гарсан бол тэтгэврийн орлогоо барьцаалж, зээлийн үйлчилгээ аваарай.

Тэтгэврийн зээл
ХХБ-аар дамжуулж тэтгэвэр авдаг ахмадуудад зориулсан уян хатан, таатай нөхцөлтэй зээлийн үйлчилгээ

Хялбар зээл
Гэнэтийн мөнгөний хэрэгцээг тань хялбар шийдэх үл хөдлөх барьцаалан олгох зээлийн үйлчилгээ.

Хадгаламж барьцаалсан зээл
ХХБ-нд хугацаатай хадгаламжтай харилцагч та өөрийн хадгаламжаа барьцаалан банкны бүх салбар болон онлайнаар авах боломжтой.
Энэ хэсэгт ХХБ-ны хэрэглээний зээл болон бусад зээлийн бүтээгдэхүүнүүдийн талаар товч мэдээлэл багтсан байна.
' metadata={'source': 'https://tdbm.mn/mn/retail/loans/heregleenii-zeel', 'relevance_score': 0.1260303}
++++++++++++++++++++++++++
page_content='Бүрдүүлэх бичиг, баримт
Хүчин төгөлдөр иргэний үнэмлэх
Худалдан авах бараа, бүтээгдэхүүний нэхэмжлэх



Та Ногоон зээлийн бүтээгдэхүүнийг сонгож, байгаль орчинд ээлтэй хэрэглээг дэмжин, дэлхийгээ хайрлаарай. Энэхүү зээл нь иргэдэд зориулагдсан бөгөөд 50 сая төгрөг хүртэлх хэмжээтэй, жилийн 15%-ийн хүүтэй, 30 сар хүртэлх хугацаатай байна. Урьдчилгаа төлбөргүй бөгөөд дансны орлого, ирээдүйд олох орлогоо барьцаалах боломжтой. Зээлийн үйлчилгээний шимтгэлгүй (Хэрэглээний цахим ногоон зээлд хамаарахгүй). Зээл авахын тулд сүүлийн 6 сар тухайн байгууллагадаа үндсэн ажилтнаар ажилласан байх шаардлагатай.

Ногоон зээлээр цахилгаан халаагуур, дулаалгын материал, бага оврын цэвэрлэх байгууламж, эко жорлон, цахилгаан машин, скүүтер, эрчим хүчний хэмнэлттэй байшин болон бүтээгдэхүүн худалдан авах боломжтой.
Ногоон зээл авахын тулд бүрдүүлэх баримт, зээлийн нөхцөл, зориулалтыг тайлбарласан хэсэг.
' metadata={'source': 'https://tdbm.mn/mn/retail/loans/heregleenii-zeel/nogoon-zeel', 'relevance_score': 0.024237635}
++++++++++++++++++++++++++
page_content='Гэрээ барьцаалсан зээл нь барьцаа хөрөнгийн дутагдалтай бизнес эрхлэгч аж ахуй нэгжийн богино хугацааны эргэлтийн хөрөнгийг санхүүжүүлэх зориулалттай бөгөөд бизнесийн эргэлтийг сайжруулж, үйлдвэрлэл үйлчилгээний цар хүрээг тэлэхэд тусална. Оюу Толгой ХХК-ийн бэлтгэн нийлүүлэгчдэд зориулсан зээл нь авлагыг санхүүжүүлж, бизнесийн эргэц, үр ашгийг нэмэгдүүлэх боломжтой. Авлага барьцаалсан зээл нь бизнесийн эргэц, үр ашгийг нэмэгдүүлэх, авлагыг бууруулах зорилготой.

Овердрафт нь түргэн шуурхай бөгөөд зээлийн материал бүрдүүлэхэд хялбар, харилцах дансны орлогыг барьцаалан санхүүгээ шийдэх боломжтой. Эмэгтэй бизнес эрхлэгчдийг дэмжих зээл нь хүү бага, хугацаа урттай, уян хатан нөхцөлтэй. Тоног төхөөрөмжийн зээл нь худалдан авч буй тоног төхөөрөмжийг барьцаалан зээл авах боломжтой бөгөөд зээлийн хугацаа болон эргэн төлөлтийг уян хатан тогтоох боломжтой.
Энэ хэсэгт гэрээ барьцаалсан, Оюу Толгой, авлага барьцаалсан зээл, овердрафт, эмэгтэйчүүдийг дэмжих, тоног төхөөрөмжийн зээлийн талаар товч тайлбарласан.
' metadata={'source': 'https://tdbm.mn/mn/corporate/loans/biznesiyn-zeel', 'relevance_score': 0.010448625}
++++++++++++++++++++++++++